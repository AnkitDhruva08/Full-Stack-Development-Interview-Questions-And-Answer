{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e9785-85cb-4528-8575-768eab7dd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------GEO Readiness & Governance---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26214b0d-1265-41e8-894b-b3c79bdb4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------ Sub-pillar 2 Crawlability & Directive Integrity--------------------------\n",
    "Purpose: Verifies robots.txt for AI and general crawler accessibility.\n",
    "Strengths: Multi-agent checks (e.g., GPTBot, ClaudeBot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3d739a-ead9-47a8-b814-3c50a167c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pillar 1, Sub-pillar 2\n",
    "# See Bridge.ipynb cell 2 for logic\n",
    "# ...existing code...\n",
    "import requests\n",
    "import urllib.robotparser\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Helper to print colored and formatted text for better readability\n",
    "def print_header(text):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def print_subheader(text):\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "def print_status(message, status):\n",
    "    # Pad message for alignment\n",
    "    padded_message = f\"{message:<45}\"\n",
    "    # Add color based on status\n",
    "    if status == \"FAIL\" or status == \"CRITICAL\":\n",
    "        status_str = f\"[\\033[91m{status}\\033[0m]\" # Red\n",
    "    elif status == \"WARN\":\n",
    "        status_str = f\"[\\033[93m{status}\\033[0m]\" # Yellow\n",
    "    elif status == \"PASS\" or status == \"INFO\":\n",
    "        status_str = f\"[\\033[92m{status}\\033[0m]\" # Green\n",
    "    else:\n",
    "        status_str = f\"[{status}]\"\n",
    "\n",
    "    print(f\"{padded_message} {status_str}\")\n",
    "\n",
    "def print_recommendation(rec):\n",
    "    print(f\"  - {rec}\")\n",
    "\n",
    "class RobotsTxtAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes a website's robots.txt for crawlability and integrity based on ARI v10.0 Pillar 1, Sub-pillar 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = self._format_base_url(base_url)\n",
    "        self.robots_url = urljoin(self.base_url, 'robots.txt')\n",
    "        self.report = {\n",
    "            \"recommendations\": [],\n",
    "            \"findings\": [],\n",
    "            \"score\": 0,\n",
    "            \"status\": \"Not Assessed\"\n",
    "        }\n",
    "        self.robots_content = None\n",
    "        self.parser = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "        # User agents to test against, including web, general AI, and specific AI crawlers\n",
    "        self.USER_AGENTS = {\n",
    "            \"Wildcard\": \"*\",\n",
    "            \"Google Search\": \"Googlebot\",\n",
    "            \"Google AI\": \"Google-Extended\",\n",
    "            \"OpenAI AI\": \"GPTBot\",\n",
    "            \"Anthropic AI\": \"anthropic-ai\"\n",
    "        }\n",
    "\n",
    "        # Common paths for critical resources\n",
    "        self.CRITICAL_PATHS = [\"/static/\", \"/assets/\", \"/css/\", \"/js/\", \"/images/\", \"/media/\"]\n",
    "\n",
    "    def _format_base_url(self, url):\n",
    "        \"\"\"Ensures the URL has a scheme and is just the base domain.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        if not parsed.scheme:\n",
    "            url = \"https://\" + url\n",
    "            parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "    def fetch_robots_txt(self):\n",
    "        \"\"\"Fetches the robots.txt file from the target domain.\"\"\"\n",
    "        print_status(f\"Fetching {self.robots_url}\", \"IN PROGRESS\")\n",
    "        try:\n",
    "            response = requests.get(self.robots_url, timeout=10, headers={'User-Agent': 'ARI-Robots-Analyzer/1.0'})\n",
    "            if response.status_code == 200:\n",
    "                self.robots_content = response.text\n",
    "                self.parser.parse(self.robots_content.splitlines())\n",
    "                print_status(f\"Successfully fetched robots.txt\", \"PASS\")\n",
    "                return True\n",
    "            else:\n",
    "                self.report[\"findings\"].append(f\"robots.txt is missing or inaccessible (Status: {response.status_code}).\")\n",
    "                print_status(f\"robots.txt inaccessible (HTTP {response.status_code})\", \"WARN\")\n",
    "                return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.report[\"findings\"].append(f\"Could not fetch robots.txt due to a network error: {e}\")\n",
    "            print_status(\"Failed to fetch robots.txt (Network Error)\", \"FAIL\")\n",
    "            return False\n",
    "\n",
    "    def check_sitemap_directive(self):\n",
    "        \"\"\"Checks for the presence of a Sitemap directive.\"\"\"\n",
    "        if self.robots_content and 'sitemap:' in self.robots_content.lower():\n",
    "            self.report[\"findings\"].append(\"Sitemap directive is present in robots.txt.\")\n",
    "            print_status(\"Sitemap directive check\", \"PASS\")\n",
    "            return True\n",
    "        else:\n",
    "            self.report[\"findings\"].append(\"Sitemap directive is missing from robots.txt.\")\n",
    "            print_status(\"Sitemap directive check\", \"WARN\")\n",
    "            return False\n",
    "\n",
    "    def check_crawlability(self):\n",
    "        \"\"\"Checks if key user agents are blocked from the root or critical paths.\"\"\"\n",
    "        is_globally_blocked = False\n",
    "\n",
    "        print_subheader(\"Agent Crawlability Analysis\")\n",
    "\n",
    "        for name, agent in self.USER_AGENTS.items():\n",
    "            # Check for root access\n",
    "            can_fetch_root = self.parser.can_fetch(agent, self.base_url + \"/\")\n",
    "            status = \"PASS\" if can_fetch_root else \"FAIL\"\n",
    "            print_status(f\"Root access for '{name}' ({agent})\", status)\n",
    "\n",
    "            if not can_fetch_root:\n",
    "                 self.report[\"findings\"].append(f\"Agent '{name}' is blocked from crawling the site root.\")\n",
    "                 if agent == '*':\n",
    "                    is_globally_blocked = True\n",
    "\n",
    "            # Check critical resource paths\n",
    "            blocked_resources = []\n",
    "            for path in self.CRITICAL_PATHS:\n",
    "                if not self.parser.can_fetch(agent, self.base_url + path):\n",
    "                    blocked_resources.append(path)\n",
    "\n",
    "            if blocked_resources:\n",
    "                status = \"FAIL\"\n",
    "                self.report[\"findings\"].append(f\"Agent '{name}' is blocked from critical resource paths: {', '.join(blocked_resources)}\")\n",
    "            else:\n",
    "                status = \"PASS\"\n",
    "            print_status(f\"Critical resource access for '{name}'\", status)\n",
    "\n",
    "        return is_globally_blocked\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Main execution logic.\"\"\"\n",
    "        print_header(\"ARI Sub-Pillar 1.2: Crawlability & Directive Integrity\")\n",
    "\n",
    "        if not self.fetch_robots_txt():\n",
    "            self.report[\"status\"] = \"Poor\"\n",
    "            self.report[\"score\"] = 40  # Not a blocker, but a significant issue\n",
    "            self.report[\"recommendations\"].append(\"Create a properly formatted robots.txt file to provide clear instructions to crawlers.\")\n",
    "            self.report[\"recommendations\"].append(\"Include a sitemap reference in the new robots.txt file.\")\n",
    "            self._print_final_report()\n",
    "            return\n",
    "\n",
    "        has_sitemap = self.check_sitemap_directive()\n",
    "        is_blocked = self.check_crawlability()\n",
    "\n",
    "        # Scoring Logic\n",
    "        if is_blocked:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 0 # BLOCKER\n",
    "            self.report[\"recommendations\"].append(\"Major Issue: The site is globally blocked (Disallow: / for User-agent: *). This is a critical barrier for all agents.\")\n",
    "        else:\n",
    "            # Start with a base score for a valid, non-blocking file\n",
    "            score = 70\n",
    "            status = \"Good\"\n",
    "\n",
    "            # Bonus for having a sitemap\n",
    "            if has_sitemap:\n",
    "                score += 20\n",
    "            else:\n",
    "                self.report[\"recommendations\"].append(\"Include a sitemap reference in robots.txt for better crawl efficiency.\")\n",
    "\n",
    "            # Penalty for any blocked critical resources\n",
    "            if any(\"blocked from critical resource\" in f for f in self.report[\"findings\"]):\n",
    "                score -= 30\n",
    "                self.report[\"recommendations\"].append(\"Allow access to critical resources (CSS, JS, images) for AI agents to ensure proper page rendering and understanding.\")\n",
    "\n",
    "            # Penalty for blocking specific (non-*) AI agents\n",
    "            if any(\"is blocked from crawling\" in f and \"Wildcard\" not in f for f in self.report[\"findings\"]):\n",
    "                score -= 15\n",
    "                self.report[\"recommendations\"].append(\"Review agent-specific directives to ensure key AI crawlers (e.g., Google-Extended, GPTBot) are not unintentionally blocked.\")\n",
    "\n",
    "            self.report[\"score\"] = max(0, score) # Ensure score doesn't go below 0\n",
    "\n",
    "            if self.report[\"score\"] >= 90:\n",
    "                self.report[\"status\"] = \"Excellent\"\n",
    "            elif self.report[\"score\"] >= 65:\n",
    "                 self.report[\"status\"] = \"Good\"\n",
    "            else:\n",
    "                 self.report[\"status\"] = \"Needs Improvement\"\n",
    "\n",
    "        # Final recommendation if the file is empty or lacks directives\n",
    "        if not self.robots_content.strip() or (\"user-agent\" not in self.robots_content.lower()):\n",
    "            self.report[\"recommendations\"].append(\"The robots.txt file is empty or malformed. Add appropriate User-agent and Disallow/Allow directives.\")\n",
    "            self.report[\"score\"] = min(self.report[\"score\"], 30) # Cap score for malformed file\n",
    "            self.report[\"status\"] = \"Poor\"\n",
    "\n",
    "        self._print_final_report()\n",
    "\n",
    "    def _print_final_report(self):\n",
    "        \"\"\"Prints the final formatted report.\"\"\"\n",
    "        print_header(\"Final Assessment Report\")\n",
    "        print_status(\"Overall Status\", self.report['status'])\n",
    "        print_status(\"ARI Score (out of 100)\", self.report['score'])\n",
    "\n",
    "        print_subheader(\"Findings\")\n",
    "        if self.report[\"findings\"]:\n",
    "            for finding in self.report[\"findings\"]:\n",
    "                print(f\"  - {finding}\")\n",
    "        else:\n",
    "            print(\"  - No significant issues found.\")\n",
    "\n",
    "        if self.report[\"recommendations\"]:\n",
    "            print_subheader(\"Recommendations (Based on ARI v10.0)\")\n",
    "            for rec in self.report[\"recommendations\"]:\n",
    "                print_recommendation(rec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e68c36b-f5bb-42a1-adcb-06c46f1e574d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL ::::  github.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ARI Sub-Pillar 1.2: Crawlability & Directive Integrity\n",
      "============================================================\n",
      "Fetching https://github.com/robots.txt        [IN PROGRESS]\n",
      "Successfully fetched robots.txt               [\u001b[92mPASS\u001b[0m]\n",
      "Sitemap directive check                       [\u001b[93mWARN\u001b[0m]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Agent Crawlability Analysis\n",
      "------------------------------------------------------------\n",
      "Root access for 'Wildcard' (*)                [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Wildcard'       [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'Google Search' (Googlebot)   [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Google Search'  [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'Google AI' (Google-Extended) [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Google AI'      [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'OpenAI AI' (GPTBot)          [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'OpenAI AI'      [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'Anthropic AI' (anthropic-ai) [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Anthropic AI'   [\u001b[92mPASS\u001b[0m]\n",
      "\n",
      "============================================================\n",
      " Final Assessment Report\n",
      "============================================================\n",
      "Overall Status                                [Good]\n",
      "ARI Score (out of 100)                        [70]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Findings\n",
      "------------------------------------------------------------\n",
      "  - Sitemap directive is missing from robots.txt.\n",
      "\n",
      "------------------------------------------------------------\n",
      " Recommendations (Based on ARI v10.0)\n",
      "------------------------------------------------------------\n",
      "  - Include a sitemap reference in robots.txt for better crawl efficiency.\n",
      "\n",
      "============================================================\n",
      "RAW REPORT OBJECT\n",
      "============================================================\n",
      "{'recommendations': ['Include a sitemap reference in robots.txt for better crawl efficiency.'], 'findings': ['Sitemap directive is missing from robots.txt.'], 'score': 70, 'status': 'Good'}\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for a URL\n",
    "target = input(\"Enter URL :::: \").strip()\n",
    "\n",
    "# Initialize the analyzer\n",
    "RTA = RobotsTxtAnalyzer(target)\n",
    "\n",
    "# Run the full analysis\n",
    "RTA.run_analysis()\n",
    "\n",
    "\n",
    "\n",
    "# Print the raw Python report object\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW REPORT OBJECT\")\n",
    "print(\"=\"*60)\n",
    "print(RTA.report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5fd1d-d598-4a0d-a10d-d273a6ad70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------WHAT IS MISSING ---------------------\n",
    "️1. Structured Findings (Major Gap)\n",
    "\n",
    "    Current: All findings are strings:\n",
    "    \n",
    "    \"Agent 'GPTBot' is blocked from critical resource paths: /static/, /css/\"\n",
    "    \n",
    "    \n",
    "    Problem: Hard for downstream tools or automated analysis.\n",
    "    \n",
    "    Fix: Use dicts per agent/path, for example:\n",
    "    \n",
    "    {\n",
    "        \"agent\": \"GPTBot\",\n",
    "        \"root_access\": False,\n",
    "        \"blocked_paths\": [\"/static/\", \"/css/\"],\n",
    "        \"status\": \"FAIL\"\n",
    "    }\n",
    "️2. Per-path Granularity\n",
    "\n",
    "    Right now, blocked resources are aggregated in one string per agent.\n",
    "    \n",
    "    Improvement: Store per-path results so you know exactly which paths are blocked/allowed.\n",
    "    This helps produce a more precise score.\n",
    "\n",
    "3️.  More Detailed Scoring\n",
    "\n",
    "    Penalties are fixed (-30 for blocked resources, -15 for specific agents).\n",
    "    \n",
    "    Improvement: Weight penalties based on:\n",
    "    \n",
    "    How many critical paths are blocked\n",
    "    \n",
    "    Number of AI agents blocked\n",
    "    \n",
    "    Global vs agent-specific block\n",
    "    \n",
    "    Makes the score more granular and realistic.\n",
    "\n",
    "️4. Sitemap Validation\n",
    "\n",
    "    Currently checks only for 'sitemap:' string.\n",
    "    \n",
    "    Improvement: Validate the URL (HTTP 200 + correct format) to ensure it’s actually reachable.\n",
    "\n",
    "5 . Empty / Malformed robots.txt Handling\n",
    "\n",
    "    already check for empty content or missing User-agent.\n",
    "    \n",
    "    Improvement: Add a structured finding for this issue, e.g.:\n",
    "    \n",
    "    {\"issue\": \"empty_or_malformed\", \"status\": \"FAIL\"}\n",
    "\n",
    "️6.  Machine-Readable Output\n",
    "\n",
    "    Currently everything is printed and recommendations are strings.\n",
    "    \n",
    "    Improvement: Add a JSON-exportable report\n",
    "\n",
    "    def export_json_report(self, path=\"robots_report.json\"):\n",
    "        import json\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.report, f, indent=2)\n",
    "\n",
    "️7. Optional Enhancements\n",
    "\n",
    "    Support custom user-agent lists from a config file or input.\n",
    "    \n",
    "    Consider asynchronous checks for performance if many agents/paths.\n",
    "    \n",
    "    Include timestamp and URL metadata in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ad0cd-71ee-4289-9a6c-be99102fd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------------------IMPROVEMENTS --------------------------------------\n",
    "Improvements in this version\n",
    "\n",
    "    1. Structured findings per agent, including:\n",
    "    \n",
    "    2. root_access, blocked_paths, status, message\n",
    "    \n",
    "    3. Per-path crawlability tracked separately.\n",
    "    \n",
    "    4. Granular scoring:\n",
    "    \n",
    "    5. Bonus for sitemap, penalties based on number of blocked paths\n",
    "    \n",
    "    6. Sitemap validation (presence check)\n",
    "    \n",
    "    7. Handles empty/malformed robots.txt properly\n",
    "    \n",
    "    8. Machine-readable JSON export\n",
    "    \n",
    "    9. Timestamps included in the report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc515dfc-39d7-401e-97f8-d5cef6b3aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pillar 1, Sub-pillar 2 — Structured Robots.txt Analyzer\n",
    "import requests\n",
    "import urllib.robotparser\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import json\n",
    "\n",
    "# Helper functions for colored CLI output\n",
    "def print_header(text):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def print_subheader(text):\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "def print_status(message, status):\n",
    "    padded_message = f\"{message:<50}\"\n",
    "    colors = {\"FAIL\":91, \"CRITICAL\":91, \"WARN\":93, \"PASS\":92, \"INFO\":92}\n",
    "    status_str = f\"[\\033[{colors.get(status,0)}m{status}\\033[0m]\"\n",
    "    print(f\"{padded_message} {status_str}\")\n",
    "\n",
    "def print_recommendation(rec):\n",
    "    print(f\"  - {rec}\")\n",
    "\n",
    "class RobotsTxtAnalyzer:\n",
    "    \"\"\"\n",
    "    ARI v10.0 Pillar 1, Sub-pillar 2\n",
    "    Checks robots.txt for crawlability, sitemap presence, and integrity.\n",
    "    Produces structured findings per agent and per path.\n",
    "    \"\"\"\n",
    "    CRITICAL_PATHS = [\"/static/\", \"/assets/\", \"/css/\", \"/js/\", \"/images/\", \"/media/\"]\n",
    "\n",
    "    DEFAULT_AGENTS = {\n",
    "        \"Wildcard\": \"*\",\n",
    "        \"Google Search\": \"Googlebot\",\n",
    "        \"Google AI\": \"Google-Extended\",\n",
    "        \"OpenAI AI\": \"GPTBot\",\n",
    "        \"Anthropic AI\": \"anthropic-ai\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, base_url, agents=None):\n",
    "        self.base_url = self._format_base_url(base_url)\n",
    "        self.robots_url = urljoin(self.base_url, \"robots.txt\")\n",
    "        self.report = {\n",
    "            \"url\": self.base_url,\n",
    "            \"status\": \"Not Assessed\",\n",
    "            \"score\": 0,\n",
    "            \"findings\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"timestamp\": None\n",
    "        }\n",
    "        self.robots_content = \"\"\n",
    "        self.parser = urllib.robotparser.RobotFileParser()\n",
    "        self.USER_AGENTS = agents or self.DEFAULT_AGENTS\n",
    "\n",
    "    def _format_base_url(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        if not parsed.scheme:\n",
    "            url = \"https://\" + url\n",
    "            parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "    def fetch_robots_txt(self):\n",
    "        print_status(f\"Fetching {self.robots_url}\", \"IN PROGRESS\")\n",
    "        try:\n",
    "            resp = requests.get(self.robots_url, timeout=10,\n",
    "                                headers={'User-Agent': 'ARI-Robots-Analyzer/1.0'})\n",
    "            if resp.status_code == 200:\n",
    "                self.robots_content = resp.text\n",
    "                self.parser.parse(self.robots_content.splitlines())\n",
    "                print_status(\"robots.txt fetched successfully\", \"PASS\")\n",
    "                return True\n",
    "            else:\n",
    "                self.report[\"findings\"].append({\n",
    "                    \"issue\": \"robots_missing\",\n",
    "                    \"status\": \"WARN\",\n",
    "                    \"http_status\": resp.status_code,\n",
    "                    \"message\": \"robots.txt missing or inaccessible\"\n",
    "                })\n",
    "                print_status(f\"robots.txt inaccessible (HTTP {resp.status_code})\", \"WARN\")\n",
    "                return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.report[\"findings\"].append({\n",
    "                \"issue\": \"robots_network_error\",\n",
    "                \"status\": \"FAIL\",\n",
    "                \"message\": str(e)\n",
    "            })\n",
    "            print_status(\"Failed to fetch robots.txt (Network Error)\", \"FAIL\")\n",
    "            return False\n",
    "\n",
    "    def check_sitemap(self):\n",
    "        if self.robots_content and \"sitemap:\" in self.robots_content.lower():\n",
    "            self.report[\"findings\"].append({\n",
    "                \"issue\": \"sitemap_present\",\n",
    "                \"status\": \"PASS\",\n",
    "                \"message\": \"Sitemap directive is present\"\n",
    "            })\n",
    "            print_status(\"Sitemap directive check\", \"PASS\")\n",
    "            return True\n",
    "        else:\n",
    "            self.report[\"findings\"].append({\n",
    "                \"issue\": \"sitemap_missing\",\n",
    "                \"status\": \"WARN\",\n",
    "                \"message\": \"Sitemap directive is missing\"\n",
    "            })\n",
    "            print_status(\"Sitemap directive check\", \"WARN\")\n",
    "            return False\n",
    "\n",
    "    def check_crawlability(self):\n",
    "        print_subheader(\"Agent Crawlability Analysis\")\n",
    "        global_block = False\n",
    "\n",
    "        for name, agent in self.USER_AGENTS.items():\n",
    "            agent_result = {\"agent\": name, \"user_agent\": agent, \"root_access\": True, \"blocked_paths\": []}\n",
    "\n",
    "            # Root access\n",
    "            can_fetch_root = self.parser.can_fetch(agent, self.base_url + \"/\")\n",
    "            agent_result[\"root_access\"] = can_fetch_root\n",
    "            if not can_fetch_root:\n",
    "                agent_result[\"status\"] = \"FAIL\"\n",
    "                agent_result[\"message\"] = \"Blocked from root\"\n",
    "                self.report[\"recommendations\"].append(\n",
    "                    f\"Allow '{name}' to crawl the root path '/'\"\n",
    "                )\n",
    "                if agent == \"*\":\n",
    "                    global_block = True\n",
    "            else:\n",
    "                agent_result[\"status\"] = \"PASS\"\n",
    "                agent_result[\"message\"] = \"Root accessible\"\n",
    "\n",
    "            print_status(f\"Root access for '{name}' ({agent})\", agent_result[\"status\"])\n",
    "\n",
    "            # Critical paths\n",
    "            for path in self.CRITICAL_PATHS:\n",
    "                if not self.parser.can_fetch(agent, self.base_url + path):\n",
    "                    agent_result[\"blocked_paths\"].append(path)\n",
    "\n",
    "            if agent_result[\"blocked_paths\"]:\n",
    "                print_status(f\"Critical resource access for '{name}'\", \"FAIL\")\n",
    "                self.report[\"recommendations\"].append(\n",
    "                    f\"Agent '{name}' blocked from critical paths: {', '.join(agent_result['blocked_paths'])}\"\n",
    "                )\n",
    "            else:\n",
    "                print_status(f\"Critical resource access for '{name}'\", \"PASS\")\n",
    "\n",
    "            self.report[\"findings\"].append(agent_result)\n",
    "\n",
    "        return global_block\n",
    "\n",
    "    def run_analysis(self):\n",
    "        import datetime\n",
    "        self.report[\"timestamp\"] = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "        print_header(\"ARI Sub-Pillar 1.2: Crawlability & Directive Integrity\")\n",
    "\n",
    "        if not self.fetch_robots_txt():\n",
    "            self.report[\"status\"] = \"Poor\"\n",
    "            self.report[\"score\"] = 40\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Create a properly formatted robots.txt file to provide clear instructions.\"\n",
    "            )\n",
    "            self._print_final_report()\n",
    "            return\n",
    "\n",
    "        has_sitemap = self.check_sitemap()\n",
    "        is_blocked = self.check_crawlability()\n",
    "\n",
    "        # Scoring\n",
    "        score = 70\n",
    "        if has_sitemap:\n",
    "            score += 20\n",
    "        if is_blocked:\n",
    "            score = 0\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Site is globally blocked (Disallow: / for User-agent: *). Critical barrier for all agents.\"\n",
    "            )\n",
    "        else:\n",
    "            # Penalties for blocked paths\n",
    "            blocked_penalty = sum(len(f.get(\"blocked_paths\", [])) for f in self.report[\"findings\"])\n",
    "            score -= min(blocked_penalty * 5, 30)  # max -30 for blocked paths\n",
    "            self.report[\"status\"] = \"Good\" if score >= 65 else \"Needs Improvement\"\n",
    "\n",
    "        # Handle empty/malformed robots.txt\n",
    "        if not self.robots_content.strip() or \"user-agent\" not in self.robots_content.lower():\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"The robots.txt file is empty or malformed. Add User-agent and Allow/Disallow directives.\"\n",
    "            )\n",
    "            score = min(score, 30)\n",
    "            self.report[\"status\"] = \"Poor\"\n",
    "\n",
    "        self.report[\"score\"] = max(0, score)\n",
    "        self._print_final_report()\n",
    "\n",
    "    def _print_final_report(self):\n",
    "        print_header(\"Final Assessment Report\")\n",
    "        print_status(\"Overall Status\", self.report[\"status\"])\n",
    "        print_status(\"ARI Score (out of 100)\", self.report[\"score\"])\n",
    "\n",
    "        print_subheader(\"Findings\")\n",
    "        for f in self.report[\"findings\"]:\n",
    "            print(json.dumps(f, indent=2))\n",
    "\n",
    "        if self.report[\"recommendations\"]:\n",
    "            print_subheader(\"Recommendations\")\n",
    "            for rec in self.report[\"recommendations\"]:\n",
    "                print_recommendation(rec)\n",
    "\n",
    "    def export_json_report(self, path=\"robots_report.json\"):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.report, f, indent=2)\n",
    "        print_status(f\"JSON report exported to {path}\", \"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "980507a5-b879-49be-8926-274accf19053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL ::::  https://buildbridges.co/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ARI Sub-Pillar 1.2: Crawlability & Directive Integrity\n",
      "============================================================\n",
      "Fetching https://buildbridges.co/robots.txt        [\u001b[0mIN PROGRESS\u001b[0m]\n",
      "robots.txt fetched successfully                    [\u001b[92mPASS\u001b[0m]\n",
      "Sitemap directive check                            [\u001b[93mWARN\u001b[0m]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Agent Crawlability Analysis\n",
      "------------------------------------------------------------\n",
      "Root access for 'Wildcard' (*)                     [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Wildcard'            [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'Google Search' (Googlebot)        [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Google Search'       [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'Google AI' (Google-Extended)      [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Google AI'           [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'OpenAI AI' (GPTBot)               [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'OpenAI AI'           [\u001b[92mPASS\u001b[0m]\n",
      "Root access for 'Anthropic AI' (anthropic-ai)      [\u001b[92mPASS\u001b[0m]\n",
      "Critical resource access for 'Anthropic AI'        [\u001b[92mPASS\u001b[0m]\n",
      "\n",
      "============================================================\n",
      " Final Assessment Report\n",
      "============================================================\n",
      "Overall Status                                     [\u001b[0mPoor\u001b[0m]\n",
      "ARI Score (out of 100)                             [\u001b[0m30\u001b[0m]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Findings\n",
      "------------------------------------------------------------\n",
      "{\n",
      "  \"issue\": \"sitemap_missing\",\n",
      "  \"status\": \"WARN\",\n",
      "  \"message\": \"Sitemap directive is missing\"\n",
      "}\n",
      "{\n",
      "  \"agent\": \"Wildcard\",\n",
      "  \"user_agent\": \"*\",\n",
      "  \"root_access\": true,\n",
      "  \"blocked_paths\": [],\n",
      "  \"status\": \"PASS\",\n",
      "  \"message\": \"Root accessible\"\n",
      "}\n",
      "{\n",
      "  \"agent\": \"Google Search\",\n",
      "  \"user_agent\": \"Googlebot\",\n",
      "  \"root_access\": true,\n",
      "  \"blocked_paths\": [],\n",
      "  \"status\": \"PASS\",\n",
      "  \"message\": \"Root accessible\"\n",
      "}\n",
      "{\n",
      "  \"agent\": \"Google AI\",\n",
      "  \"user_agent\": \"Google-Extended\",\n",
      "  \"root_access\": true,\n",
      "  \"blocked_paths\": [],\n",
      "  \"status\": \"PASS\",\n",
      "  \"message\": \"Root accessible\"\n",
      "}\n",
      "{\n",
      "  \"agent\": \"OpenAI AI\",\n",
      "  \"user_agent\": \"GPTBot\",\n",
      "  \"root_access\": true,\n",
      "  \"blocked_paths\": [],\n",
      "  \"status\": \"PASS\",\n",
      "  \"message\": \"Root accessible\"\n",
      "}\n",
      "{\n",
      "  \"agent\": \"Anthropic AI\",\n",
      "  \"user_agent\": \"anthropic-ai\",\n",
      "  \"root_access\": true,\n",
      "  \"blocked_paths\": [],\n",
      "  \"status\": \"PASS\",\n",
      "  \"message\": \"Root accessible\"\n",
      "}\n",
      "\n",
      "------------------------------------------------------------\n",
      " Recommendations\n",
      "------------------------------------------------------------\n",
      "  - The robots.txt file is empty or malformed. Add User-agent and Allow/Disallow directives.\n",
      "JSON report exported to robots_report.json         [\u001b[92mINFO\u001b[0m]\n",
      "\n",
      "============================================================\n",
      "RAW REPORT OBJECT\n",
      "============================================================\n",
      "{\n",
      "  \"url\": \"https://buildbridges.co\",\n",
      "  \"status\": \"Poor\",\n",
      "  \"score\": 30,\n",
      "  \"findings\": [\n",
      "    {\n",
      "      \"issue\": \"sitemap_missing\",\n",
      "      \"status\": \"WARN\",\n",
      "      \"message\": \"Sitemap directive is missing\"\n",
      "    },\n",
      "    {\n",
      "      \"agent\": \"Wildcard\",\n",
      "      \"user_agent\": \"*\",\n",
      "      \"root_access\": true,\n",
      "      \"blocked_paths\": [],\n",
      "      \"status\": \"PASS\",\n",
      "      \"message\": \"Root accessible\"\n",
      "    },\n",
      "    {\n",
      "      \"agent\": \"Google Search\",\n",
      "      \"user_agent\": \"Googlebot\",\n",
      "      \"root_access\": true,\n",
      "      \"blocked_paths\": [],\n",
      "      \"status\": \"PASS\",\n",
      "      \"message\": \"Root accessible\"\n",
      "    },\n",
      "    {\n",
      "      \"agent\": \"Google AI\",\n",
      "      \"user_agent\": \"Google-Extended\",\n",
      "      \"root_access\": true,\n",
      "      \"blocked_paths\": [],\n",
      "      \"status\": \"PASS\",\n",
      "      \"message\": \"Root accessible\"\n",
      "    },\n",
      "    {\n",
      "      \"agent\": \"OpenAI AI\",\n",
      "      \"user_agent\": \"GPTBot\",\n",
      "      \"root_access\": true,\n",
      "      \"blocked_paths\": [],\n",
      "      \"status\": \"PASS\",\n",
      "      \"message\": \"Root accessible\"\n",
      "    },\n",
      "    {\n",
      "      \"agent\": \"Anthropic AI\",\n",
      "      \"user_agent\": \"anthropic-ai\",\n",
      "      \"root_access\": true,\n",
      "      \"blocked_paths\": [],\n",
      "      \"status\": \"PASS\",\n",
      "      \"message\": \"Root accessible\"\n",
      "    }\n",
      "  ],\n",
      "  \"recommendations\": [\n",
      "    \"The robots.txt file is empty or malformed. Add User-agent and Allow/Disallow directives.\"\n",
      "  ],\n",
      "  \"timestamp\": \"2025-11-24T12:27:41.089330+00:00\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def analyze_robots_txt(url, export_path=None):\n",
    "    \"\"\"\n",
    "    Runs the full robots.txt analysis for the given URL.\n",
    "    Optionally exports a structured JSON report.\n",
    "    Returns the report object.\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = RobotsTxtAnalyzer(url)\n",
    "\n",
    "    # Run analysis\n",
    "    analyzer.run_analysis()\n",
    "\n",
    "    # Export JSON if path provided\n",
    "    if export_path:\n",
    "        analyzer.export_json_report(export_path)\n",
    "\n",
    "    # Return raw report object\n",
    "    return analyzer.report\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example function call\n",
    "# ---------------------------\n",
    "target_url = input(\"Enter URL :::: \").strip()\n",
    "report = analyze_robots_txt(target_url, export_path=\"robots_report.json\")\n",
    "\n",
    "# Print raw report nicely\n",
    "import json\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW REPORT OBJECT\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(report, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497183ef-3af8-4144-b141-6a845c8a36f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
