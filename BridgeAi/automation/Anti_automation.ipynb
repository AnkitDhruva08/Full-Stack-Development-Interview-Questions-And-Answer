{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ec103-d229-4836-92a5-d87ab4c5089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Aotomation Pillar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374be0f-d7a4-4817-9bb1-a706ad73c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anti_automation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7d4f5d-f5cb-45eb-a587-8605ed56a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Tuple, List\n",
    "\n",
    "# ---------- Anti-Automation Absence Check ----------\n",
    "\n",
    "def check_anti_automation_absence(url: str, html: str) -> Tuple[int, List[str], List[str]]:\n",
    "    score = 0\n",
    "    issues = []\n",
    "    recommendations = []\n",
    "\n",
    "    try:\n",
    "        # 1. Check for bot-blocking headers\n",
    "        response = requests.get(url, timeout=10)\n",
    "        headers = response.headers\n",
    "\n",
    "        if \"x-robots-tag\" not in headers or \"noindex\" not in headers.get(\"x-robots-tag\", \"\").lower():\n",
    "            score += 1\n",
    "        else:\n",
    "            issues.append(\"Site blocks bots via X-Robots-Tag header.\")\n",
    "            recommendations.append(\"Remove aggressive noindex/nofollow unless essential.\")\n",
    "\n",
    "        # 2. Check for CAPTCHAs on homepage\n",
    "        if \"captcha\" in response.text.lower():\n",
    "            issues.append(\"Potential CAPTCHA challenge found on homepage.\")\n",
    "            recommendations.append(\"Use CAPTCHAs only on sensitive actions like signups or payments.\")\n",
    "        else:\n",
    "            score += 2\n",
    "\n",
    "        # 3. robots.txt check\n",
    "        parsed = requests.utils.urlparse(url)\n",
    "        robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "        try:\n",
    "            r_txt = requests.get(robots_url, timeout=5)\n",
    "            if \"disallow: /\" not in r_txt.text.lower():\n",
    "                score += 1\n",
    "            else:\n",
    "                issues.append(\"robots.txt blocks general crawling.\")\n",
    "                recommendations.append(\"Update robots.txt to allow general crawling.\")\n",
    "        except:\n",
    "            issues.append(\"robots.txt not found or unreachable.\")\n",
    "            recommendations.append(\"Ensure robots.txt is accessible and well-configured.\")\n",
    "\n",
    "        # 4. Check for fingerprinting scripts (basic heuristic)\n",
    "        if any(s in response.text.lower() for s in [\"fingerprintjs\", \"navigator.plugins\", \"navigator.hardwareconcurrency\"]):\n",
    "            issues.append(\"Potential fingerprinting scripts detected.\")\n",
    "            recommendations.append(\"Avoid aggressive fingerprinting that blocks automation.\")\n",
    "        else:\n",
    "            score += 1\n",
    "\n",
    "        return min(score, 5), issues, recommendations\n",
    "\n",
    "    except Exception as e:\n",
    "        issues.append(\"Anti-automation check failed.\")\n",
    "        recommendations.append(str(e))\n",
    "        return 0, issues, recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d815dd-670d-45d6-b026-86328dfe28d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL to check Anti-Automation Signals:  https://www.google.com/search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ANTI-AUTOMATION ANALYSIS =====\n",
      "Final Score: 4/5\n",
      "\n",
      "Issues Found:\n",
      " - robots.txt blocks general crawling.\n",
      "\n",
      "Recommendations:\n",
      " - Update robots.txt to allow general crawling.\n"
     ]
    }
   ],
   "source": [
    "# checking what types works its doing \n",
    "url = input(\"Enter URL to check Anti-Automation Signals: \")\n",
    "\n",
    "try:\n",
    "    html = requests.get(url, timeout=10).text\n",
    "    \n",
    "    score, issues, recommendations = check_anti_automation_absence(url, html)\n",
    "\n",
    "    print(\"\\n===== ANTI-AUTOMATION ANALYSIS =====\")\n",
    "    print(f\"Final Score: {score}/5\\n\")\n",
    "\n",
    "    print(\"Issues Found:\")\n",
    "    if issues:\n",
    "        for i in issues:\n",
    "            print(\" -\", i)\n",
    "    else:\n",
    "        print(\" No issues detected.\")\n",
    "\n",
    "    print(\"\\nRecommendations:\")\n",
    "    if recommendations:\n",
    "        for r in recommendations:\n",
    "            print(\" -\", r)\n",
    "    else:\n",
    "        print(\" No recommendations — site looks automation-friendly.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error fetching URL:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a03270-1bde-4491-a83d-3c40b9e4905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Tuple, List\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ============================================================\n",
    "#      ADVANCED ANTI-AUTOMATION ABSENCE CHECK  (v2.0)\n",
    "#        Fully aligned with ARI Anti-Automation Pillar\n",
    "# ============================================================\n",
    "\n",
    "def check_anti_automation_absence(url: str, html: str) -> Tuple[int, List[str], List[str]]:\n",
    "    score = 0\n",
    "    issues = []\n",
    "    recommendations = []\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 1. User-Agent Variance Test (NEW)\n",
    "    # --------------------------------------\n",
    "    user_agents = {\n",
    "        \"desktop\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"mobile\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0)\",\n",
    "        \"bot\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n",
    "    }\n",
    "    \n",
    "    responses = {}\n",
    "    for key, ua in user_agents.items():\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10, headers={\"User-Agent\": ua})\n",
    "            responses[key] = r\n",
    "        except:\n",
    "            responses[key] = None\n",
    "\n",
    "    # If desktop loads but bot UA fails → bot-blocking detected\n",
    "    if responses[\"desktop\"] and (not responses[\"bot\"] or responses[\"bot\"].status_code >= 400):\n",
    "        issues.append(\"Site behaves differently for bot user-agents (bot-blocking likely).\")\n",
    "        recommendations.append(\"Allow consistent content for bot UAs where safe.\")\n",
    "    else:\n",
    "        score += 1\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 2. Rate-Limit & Throttling Detection (NEW)\n",
    "    # --------------------------------------\n",
    "    main_res = responses[\"desktop\"]\n",
    "    if not main_res:\n",
    "        issues.append(\"URL unreachable.\")\n",
    "        recommendations.append(\"Retry or ensure domain resolves.\")\n",
    "        return 0, issues, recommendations\n",
    "\n",
    "    if main_res.status_code == 429:\n",
    "        issues.append(\"Rate limiting detected (429 Too Many Requests).\")\n",
    "        recommendations.append(\"Increase rate limits or whitelist automation agents.\")\n",
    "    else:\n",
    "        score += 1\n",
    "\n",
    "    # Soft throttling\n",
    "    if \"retry-after\" in main_res.headers:\n",
    "        issues.append(\"Throttling detected via Retry-After header.\")\n",
    "        recommendations.append(\"Tune throttling rules to support agent flows.\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 3. Bot-Blocking Headers (IMPROVED)\n",
    "    # --------------------------------------\n",
    "    xr = main_res.headers.get(\"x-robots-tag\", \"\").lower()\n",
    "\n",
    "    if any(tag in xr for tag in [\"noindex\", \"nofollow\"]):\n",
    "        issues.append(\"Bot-blocking X-Robots-Tag detected.\")\n",
    "        recommendations.append(\"Remove restrictive directives if discovery is required.\")\n",
    "    else:\n",
    "        score += 1\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 4. robots.txt Smart Analysis (IMPROVED)\n",
    "    # --------------------------------------\n",
    "    try:\n",
    "        parsed = requests.utils.urlparse(url)\n",
    "        robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "        robots = requests.get(robots_url, timeout=5)\n",
    "\n",
    "        # Missing robots.txt = neutral (not a problem)\n",
    "        if robots.status_code != 200:\n",
    "            recommendations.append(\"robots.txt missing — optional but recommended.\")\n",
    "        else:\n",
    "            txt = robots.text.lower()\n",
    "\n",
    "            if \"disallow: /\" in txt:\n",
    "                issues.append(\"robots.txt fully blocks all crawling.\")\n",
    "                recommendations.append(\"Remove 'Disallow: /' for agents that require access.\")\n",
    "            else:\n",
    "                score += 1\n",
    "\n",
    "    except:\n",
    "        recommendations.append(\"Could not fetch robots.txt — treat as neutral.\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 5. CAPTCHA Scanner (Improved)\n",
    "    # --------------------------------------\n",
    "    lower_html = main_res.text.lower()\n",
    "    captcha_keywords = [\n",
    "        \"g-recaptcha\", \"h-captcha\", \"cf-challenge\",\n",
    "        \"turnstile\", \"please verify you are a human\",\n",
    "        \"are you a robot\"\n",
    "    ]\n",
    "\n",
    "    if any(c in lower_html for c in captcha_keywords):\n",
    "        issues.append(\"Possible CAPTCHA or challenge script found.\")\n",
    "        recommendations.append(\"Avoid CAPTCHA on navigation flows.\")\n",
    "    else:\n",
    "        score += 1\n",
    "\n",
    "    # --------------------------------------\n",
    "    # 6. Fingerprinting / Bot-Manager Detection (IMPROVED)\n",
    "    # --------------------------------------\n",
    "    fingerprint_signals = [\n",
    "        \"fingerprintjs\", \"perimeterx\", \"arkose\",\n",
    "        \"humansecurity\", \"cf-ray\", \"bot-detection\",\n",
    "        \"navigator.hardwareconcurrency\", \"webdriver\"\n",
    "    ]\n",
    "\n",
    "    if any(sig in lower_html for sig in fingerprint_signals):\n",
    "        issues.append(\"Possible bot-detection or fingerprinting libraries detected.\")\n",
    "        recommendations.append(\"Ensure fingerprinting does not block legitimate agents.\")\n",
    "    else:\n",
    "        score += 1\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Final scoring (0–5)\n",
    "    # --------------------------------------\n",
    "    score = min(score, 5)\n",
    "    return score, issues, recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f4d9dd6-c54e-4cbb-b367-2d40c010470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL to check Anti-Automation Signals:  https://www.google.com/search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ANTI-AUTOMATION ANALYSIS =====\n",
      "Final Score: 4/5\n",
      "\n",
      "Issues Found:\n",
      " - robots.txt blocks general crawling.\n",
      "\n",
      "Recommendations:\n",
      " - Update robots.txt to allow general crawling.\n"
     ]
    }
   ],
   "source": [
    "# checking what types works its doing  in new recomendation\n",
    "url = input(\"Enter URL to check Anti-Automation Signals: \")\n",
    "\n",
    "try:\n",
    "    html = requests.get(url, timeout=10).text\n",
    "    \n",
    "    score, issues, recommendations = check_anti_automation_absence(url, html)\n",
    "\n",
    "    print(\"\\n===== ANTI-AUTOMATION ANALYSIS =====\")\n",
    "    print(f\"Final Score: {score}/5\\n\")\n",
    "\n",
    "    print(\"Issues Found:\")\n",
    "    if issues:\n",
    "        for i in issues:\n",
    "            print(\" -\", i)\n",
    "    else:\n",
    "        print(\" No issues detected.\")\n",
    "\n",
    "    print(\"\\nRecommendations:\")\n",
    "    if recommendations:\n",
    "        for r in recommendations:\n",
    "            print(\" -\", r)\n",
    "    else:\n",
    "        print(\" No recommendations — site looks automation-friendly.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error fetching URL:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1d210-ba65-4664-b017-2c59e68d0349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c279f-7eb9-4607-8e20-c0d2c0cfb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038528b-345d-42f3-9c55-60bf14931fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
