{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61510cd-5667-4577-8ab3-504ae15669d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEO Readiness & Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c97f6-b910-4200-9539-e51a4e87f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "----------------------------Canonicalization---------------------------\n",
    "Canonicalization = selecting one â€œofficialâ€ URL for a piece of content when the same content is accessible through multiple URLs.\n",
    "    All show the same content, but to AI agents,\n",
    "    crawlers, and search engines these can look like different pages unless you specify the canonical.\n",
    "\n",
    "Canonicalization ensures machines know exactly which version of a page is the source of truth â€” boosting trust, \n",
    "authority, and retrieval quality.\n",
    "\n",
    "1. Why It Exists :::\n",
    "\n",
    "    Because the web is messy. Most websites unintentionally generate duplicate content through:\n",
    "    \n",
    "        1. Tracking parameters\n",
    "        \n",
    "        2. HTTP vs HTTPS\n",
    "        \n",
    "        3. WWW vs non-WWW\n",
    "        \n",
    "        4. Pagination\n",
    "        \n",
    "        5. Filtering\n",
    "        \n",
    "        6. CMS auto-routes\n",
    "        \n",
    "        7. Session IDs\n",
    "        \n",
    "        8. Internal links pointing to slightly different URLs\n",
    "    \n",
    "    This creates multiple versions of the same page. That breaks:\n",
    "    \n",
    "        1. Trust\n",
    "        \n",
    "        2. Ranking\n",
    "        \n",
    "        3. Crawl budgets\n",
    "        \n",
    "        4. AI agent confidence\n",
    "        \n",
    "        5. Data consistency\n",
    "    \n",
    "    Canonicalization solves this.\n",
    "\n",
    "\n",
    "Why GEO Readiness Cares : \n",
    "\n",
    "    AI agents (browsers, scrapers, model retrievers, search LLMs, ranking systems) need to know:\n",
    "\n",
    "    â€œWhich is the true, authoritative, machine-trusted version of this content?â€\n",
    "\n",
    "    Canonicalization gives a clear trust signal:\n",
    "\n",
    "    \"This URL is the master source. Use this.\"\n",
    "\n",
    "    This increases machine certainty\n",
    "\n",
    "\n",
    "Purpose in Enhancing Machine- Trust :::::\n",
    "    \n",
    "    Canonicalization improves:\n",
    "    \n",
    "    1. Content Authority\n",
    "    \n",
    "    Agents know which version is official â†’ increases trust and confidence.\n",
    "    \n",
    "    2. Deduplication\n",
    "    \n",
    "    Avoids agents thinking you have 10 copies of the same thing.\n",
    "    \n",
    "    3. Stronger Indexing / Retrieval\n",
    "    \n",
    "    Search engines and AI crawlers allocate more reliable embeddings and ranking signals to your canonical.\n",
    "    \n",
    "    4. Better Policy Mapping\n",
    "    \n",
    "    Your llm.txt, agents.json, and metadata only need to be aligned with one URL â€” the canonical.\n",
    "    \n",
    "    5. Reduced Fragmentation\n",
    "    \n",
    "    Models donâ€™t embed multiple versions of the same page with minor differences.\n",
    "    \n",
    "    6. Crawl Budget Efficiency\n",
    "    \n",
    "    Bots spend more time on important pages and not duplicates.\n",
    "\n",
    "\n",
    "\n",
    "What Outcome/Enhancement You Get After Implementing ::::\n",
    "\n",
    "we can confidently expect:\n",
    "\n",
    "        Outcome 1: Higher Trust in AI Agents\n",
    "        \n",
    "        Your site will be seen as clean, unambiguous, authoritative.\n",
    "        \n",
    "        Outcome 2: Better Content Ranking & Retrieval\n",
    "        \n",
    "        LLMs map your content into embeddings more accurately â†’ better surfacing.\n",
    "        \n",
    "        Outcome 3: Stronger Domain Identity\n",
    "        \n",
    "        Reduces noise and consolidates signals to the same â€œmainâ€ URL.\n",
    "        \n",
    "        Outcome 4: More Reliable Machine Understanding\n",
    "        \n",
    "        Agents can infer:\n",
    "        \n",
    "        primary topic\n",
    "        \n",
    "        primary entity\n",
    "        \n",
    "        primary author\n",
    "        \n",
    "        version to monitor\n",
    "        \n",
    "        policies to apply\n",
    "        \n",
    "        updates to track\n",
    "        \n",
    "        Outcome 5: Better GEO Score\n",
    "        \n",
    "        Canonicalization is a core mechanical integrity factor.\n",
    "        Bad canonicalization = weak trust & governance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eace96-4e9f-4ae1-a799-7e8769225c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonicalization.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c043365d-1a25-4286-ae5a-444c82674f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pillar 1, Sub-pillar 3\n",
    "# See Bridge.ipynb cell 3 for logic\n",
    "# ...existing code...\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse, urljoin, urldefrag\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import collections\n",
    "\n",
    "# Helper to print colored and formatted text for better readability\n",
    "def print_header(text):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def print_subheader(text):\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "def print_status(message, status):\n",
    "    padded_message = f\"{message:<45}\"\n",
    "    if status == \"FAIL\" or status == \"CRITICAL\":\n",
    "        status_str = f\"[\\033[91m{status}\\033[0m]\" # Red\n",
    "    elif status == \"WARN\":\n",
    "        status_str = f\"[\\033[93m{status}\\033[0m]\" # Yellow\n",
    "    elif status == \"PASS\" or status == \"INFO\":\n",
    "        status_str = f\"[\\033[92m{status}\\033[0m]\" # Green\n",
    "    else:\n",
    "        status_str = f\"[{status}]\"\n",
    "\n",
    "    print(f\"{padded_message} {status_str}\")\n",
    "\n",
    "def print_recommendation(rec):\n",
    "    print(f\"  - {rec}\")\n",
    "\n",
    "class CanonicalizationAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes a website's canonicalization and source singularity based on ARI v10.0 Pillar 1, Sub-pillar 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, max_pages_to_check=25):\n",
    "        self.base_url = self._format_base_url(base_url)\n",
    "        self.max_pages_to_check = max_pages_to_check\n",
    "        self.urls_to_check = collections.deque()\n",
    "        self.checked_urls = set()\n",
    "        self.report = {\n",
    "            \"pages_checked\": 0,\n",
    "            \"pages_with_canonical\": 0,\n",
    "            \"pages_with_absolute_canonical\": 0,\n",
    "            \"pages_with_valid_canonical_target\": 0,\n",
    "            \"issues\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"score\": 0,\n",
    "            \"status\": \"Not Assessed\"\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'ARI-Canonical-Analyzer/1.0',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        })\n",
    "\n",
    "    def _format_base_url(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        if not parsed.scheme:\n",
    "            url = \"https://\" + url\n",
    "            parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "    def _fetch_url(self, url, method='GET'):\n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = self.session.get(url, timeout=10, allow_redirects=True)\n",
    "            elif method == 'HEAD':\n",
    "                response = self.session.head(url, timeout=10, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.report[\"issues\"].append(f\"Network error for {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_urls_from_sitemap(self):\n",
    "        \"\"\"Tries to get a list of URLs from the sitemap.\"\"\"\n",
    "        print_status(\"Attempting to fetch URLs from sitemap\", \"INFO\")\n",
    "        sitemap_url = urljoin(self.base_url, 'sitemap.xml')\n",
    "        response = self._fetch_url(sitemap_url)\n",
    "        if not response:\n",
    "            print_status(\"Could not find or fetch sitemap.xml\", \"WARN\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            content = response.content\n",
    "            if response.url.endswith('.gz') or 'gzip' in response.headers.get('Content-Type', ''):\n",
    "                content = gzip.decompress(content)\n",
    "\n",
    "            root = ET.fromstring(content)\n",
    "            namespace = root.tag.split('}')[0][1:] if '}' in root.tag else ''\n",
    "\n",
    "            # Simple parser for urlset, not handling sitemap indexes for this focused check\n",
    "            for url_node in root.findall(f'.//{{{namespace}}}loc'):\n",
    "                if len(self.urls_to_check) < self.max_pages_to_check:\n",
    "                    self.urls_to_check.append(url_node.text.strip())\n",
    "            print_status(f\"Found {len(self.urls_to_check)} URLs in sitemap\", \"PASS\")\n",
    "        except Exception as e:\n",
    "            self.report[\"issues\"].append(f\"Failed to parse sitemap.xml: {e}\")\n",
    "            print_status(\"Sitemap parsing failed\", \"WARN\")\n",
    "\n",
    "    def analyze_page(self, url):\n",
    "        \"\"\"Analyzes a single page for its canonical tag.\"\"\"\n",
    "        print(f\"\\n-> Checking: {url}\")\n",
    "        self.report[\"pages_checked\"] += 1\n",
    "\n",
    "        response = self._fetch_url(url, 'GET')\n",
    "        if not response or 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "            print_status(\"Page is not valid HTML or is unreachable\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        canonical_tag = soup.find('link', {'rel': 'canonical'})\n",
    "\n",
    "        if not canonical_tag:\n",
    "            self.report[\"issues\"].append(f\"Missing canonical tag on: {url}\")\n",
    "            print_status(\"Canonical tag presence\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        self.report[\"pages_with_canonical\"] += 1\n",
    "        print_status(\"Canonical tag presence\", \"PASS\")\n",
    "\n",
    "        href = canonical_tag.get('href')\n",
    "        if not href:\n",
    "            self.report[\"issues\"].append(f\"Canonical tag has empty href on: {url}\")\n",
    "            print_status(\"Canonical href validity\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        # *** THIS IS THE CORRECTED LINE ***\n",
    "        parsed_href = urlparse(href)\n",
    "        if not (parsed_href.scheme and parsed_href.netloc):\n",
    "            self.report[\"issues\"].append(f\"Relative canonical URL '{href}' found on: {url}\")\n",
    "            print_status(\"Canonical URL is absolute\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        self.report[\"pages_with_absolute_canonical\"] += 1\n",
    "        print_status(\"Canonical URL is absolute\", \"PASS\")\n",
    "\n",
    "        # Check if the canonical target is accessible\n",
    "        clean_href = urldefrag(href).url # Remove fragments\n",
    "        head_response = self._fetch_url(clean_href, 'HEAD')\n",
    "        if head_response and head_response.status_code == 200:\n",
    "            self.report[\"pages_with_valid_canonical_target\"] += 1\n",
    "            print_status(\"Canonical target accessibility\", \"PASS\")\n",
    "        else:\n",
    "            status = head_response.status_code if head_response else 'Unreachable'\n",
    "            self.report[\"issues\"].append(f\"Canonical URL '{href}' is not accessible (Status: {status}) on page: {url}\")\n",
    "            print_status(\"Canonical target accessibility\", \"FAIL\")\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Main execution logic.\"\"\"\n",
    "        print_header(\"ARI Sub-Pillar 1.3: Canonicalization & Source Singularity\")\n",
    "\n",
    "        self.get_urls_from_sitemap()\n",
    "        if not self.urls_to_check:\n",
    "            self.urls_to_check.append(self.base_url)\n",
    "\n",
    "        print_subheader(f\"Analyzing up to {self.max_pages_to_check} pages\")\n",
    "        while self.urls_to_check and len(self.checked_urls) < self.max_pages_to_check:\n",
    "            url = self.urls_to_check.popleft()\n",
    "            if url in self.checked_urls:\n",
    "                continue\n",
    "\n",
    "            self.checked_urls.add(url)\n",
    "            self.analyze_page(url)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        self._generate_final_report()\n",
    "        self._print_final_report()\n",
    "\n",
    "    def _generate_final_report(self):\n",
    "        \"\"\"Calculate final score and populate recommendations.\"\"\"\n",
    "        if self.report[\"pages_checked\"] == 0:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 0\n",
    "            self.report[\"recommendations\"].append(\"Could not fetch or analyze any pages from the target URL.\")\n",
    "            return\n",
    "\n",
    "        # Calculate percentages\n",
    "        p_canonical = self.report[\"pages_with_canonical\"] / self.report[\"pages_checked\"]\n",
    "        p_absolute = self.report[\"pages_with_absolute_canonical\"] / self.report[\"pages_with_canonical\"] if self.report[\"pages_with_canonical\"] > 0 else 1\n",
    "        p_valid_target = self.report[\"pages_with_valid_canonical_target\"] / self.report[\"pages_with_canonical\"] if self.report[\"pages_with_canonical\"] > 0 else 1\n",
    "\n",
    "        # Scoring: 50% for presence, 25% for being absolute, 25% for being valid.\n",
    "        score = (p_canonical * 50) + (p_canonical * p_absolute * 25) + (p_canonical * p_valid_target * 25)\n",
    "        self.report[\"score\"] = int(score)\n",
    "\n",
    "        if score >= 90: self.report[\"status\"] = \"Excellent\"\n",
    "        elif score >= 70: self.report[\"status\"] = \"Good\"\n",
    "        elif score >= 40: self.report[\"status\"] = \"Needs Improvement\"\n",
    "        else: self.report[\"status\"] = \"Poor\"\n",
    "\n",
    "        # Recommendations\n",
    "        if p_canonical < 1.0:\n",
    "            self.report[\"recommendations\"].append(\"Implement <link rel='canonical'> tags on all indexable pages.\")\n",
    "        if p_absolute < 1.0:\n",
    "            self.report[\"recommendations\"].append(\"Ensure all canonical tag href attributes use absolute URLs, not relative paths.\")\n",
    "        if p_valid_target < 1.0:\n",
    "            self.report[\"recommendations\"].append(\"Audit canonical URLs that are broken (e.g., 404s) or redirect, and point them to valid, final destinations.\")\n",
    "        if score < 70:\n",
    "             self.report[\"recommendations\"].append(\"Audit for duplicate content across different URL structures (e.g., www vs. non-www, http vs. https).\")\n",
    "\n",
    "\n",
    "    def _print_final_report(self):\n",
    "        \"\"\"Prints the final formatted report.\"\"\"\n",
    "        print_header(\"Final Assessment Report\")\n",
    "        print_status(\"Overall Status\", self.report['status'])\n",
    "        print_status(\"ARI Score (out of 100)\", self.report['score'])\n",
    "\n",
    "        print_subheader(\"Summary Statistics\")\n",
    "        print(f\"Pages Analyzed: {self.report['pages_checked']}\")\n",
    "        if self.report['pages_checked'] > 0:\n",
    "            p_canonical = (self.report['pages_with_canonical'] / self.report['pages_checked']) * 100\n",
    "            print(f\"Pages with a Canonical Tag: {self.report['pages_with_canonical']}/{self.report['pages_checked']} ({p_canonical:.1f}%)\")\n",
    "        if self.report['pages_with_canonical'] > 0:\n",
    "            p_absolute = (self.report['pages_with_absolute_canonical'] / self.report['pages_with_canonical']) * 100\n",
    "            p_valid = (self.report['pages_with_valid_canonical_target'] / self.report['pages_with_canonical']) * 100\n",
    "            print(f\"Absolute Canonical URLs: {self.report['pages_with_absolute_canonical']}/{self.report['pages_with_canonical']} ({p_absolute:.1f}%)\")\n",
    "            print(f\"Accessible Canonical Targets: {self.report['pages_with_valid_canonical_target']}/{self.report['pages_with_canonical']} ({p_valid:.1f}%)\")\n",
    "\n",
    "        if self.report[\"issues\"]:\n",
    "            print_subheader(\"Detected Issues\")\n",
    "            for issue in self.report[\"issues\"][:5]: # Show top 5 issues\n",
    "                print(f\"  - {issue}\")\n",
    "\n",
    "        if self.report[\"recommendations\"]:\n",
    "            print_subheader(\"Recommendations (Based on ARI v10.0)\")\n",
    "            for rec in self.report[\"recommendations\"]:\n",
    "                print_recommendation(rec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423fac62-4143-413d-9d23-6fddc63f7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CanonicalizationAnalyzer============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:  http://localhost:3000/ari?pillar=geo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ARI Sub-Pillar 1.3: Canonicalization & Source Singularity\n",
      "============================================================\n",
      "Attempting to fetch URLs from sitemap         [\u001b[92mINFO\u001b[0m]\n",
      "Sitemap parsing failed                        [\u001b[93mWARN\u001b[0m]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Analyzing up to 25 pages\n",
      "------------------------------------------------------------\n",
      "\n",
      "-> Checking: http://localhost:3000\n",
      "Canonical tag presence                        [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "============================================================\n",
      " Final Assessment Report\n",
      "============================================================\n",
      "Overall Status                                [Poor]\n",
      "ARI Score (out of 100)                        [0]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Summary Statistics\n",
      "------------------------------------------------------------\n",
      "Pages Analyzed: 1\n",
      "Pages with a Canonical Tag: 0/1 (0.0%)\n",
      "\n",
      "------------------------------------------------------------\n",
      " Detected Issues\n",
      "------------------------------------------------------------\n",
      "  - Failed to parse sitemap.xml: syntax error: line 1, column 0\n",
      "  - Missing canonical tag on: http://localhost:3000\n",
      "\n",
      "------------------------------------------------------------\n",
      " Recommendations (Based on ARI v10.0)\n",
      "------------------------------------------------------------\n",
      "  - Implement <link rel='canonical'> tags on all indexable pages.\n",
      "  - Audit for duplicate content across different URL structures (e.g., www vs. non-www, http vs. https).\n",
      "\n",
      "============================================================\n",
      "RAW REPORT DATA (for debugging / API use)\n",
      "============================================================\n",
      "{'pages_checked': 1, 'pages_with_canonical': 0, 'pages_with_absolute_canonical': 0, 'pages_with_valid_canonical_target': 0, 'issues': ['Failed to parse sitemap.xml: syntax error: line 1, column 0', 'Missing canonical tag on: http://localhost:3000'], 'recommendations': [\"Implement <link rel='canonical'> tags on all indexable pages.\", 'Audit for duplicate content across different URL structures (e.g., www vs. non-www, http vs. https).'], 'score': 0, 'status': 'Poor'}\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN PROGRAM ---\n",
    "\n",
    "print(\"CanonicalizationAnalyzer\" + \"=\"*60)\n",
    "\n",
    "target_url = input(\"Enter URL: \")\n",
    "\n",
    "# Instantiate the analyzer with the given target URL\n",
    "analyzer = CanonicalizationAnalyzer(base_url=target_url, max_pages_to_check=25)\n",
    "\n",
    "# Run the full analysis\n",
    "analyzer.run_analysis()\n",
    "\n",
    "# Print the raw report dict if needed\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW REPORT DATA (for debugging / API use)\")\n",
    "print(\"=\"*60)\n",
    "print(analyzer.report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf377c-be4b-44ce-854c-db9a773dcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------WHAT IS MISSING IN CURRENT CODE -----------------------\n",
    "1ï¸âƒ£ NO CHECK for Canonical SELF-REFERENCE (CRITICAL)\n",
    "\n",
    "        Every page should canonicalize to itself, unless it is:\n",
    "        \n",
    "       A. paginated\n",
    "        \n",
    "       B. filtered\n",
    "        \n",
    "       C. duplicate variant\n",
    "        \n",
    "       D. alternate-language\n",
    "        \n",
    "       E. A/B test bucket\n",
    "        \n",
    "        Right now,  code never checks this.\n",
    "        \n",
    "        WE MUST verify:\n",
    "        clean_current = urldefrag(response.url).url\n",
    "        clean_canonical = urldefrag(href).url\n",
    "        \n",
    "        if clean_current != clean_canonical:\n",
    "            flag â€œnon-self-referencing canonicalâ€\n",
    "        \n",
    "        \n",
    "        This is a major signal of duplicate content.\n",
    "\n",
    "2ï¸âƒ£ NO CHECK for MULTIPLE CANONICAL TAGS (CRITICAL)\n",
    "\n",
    "    If a page has:\n",
    "    \n",
    "    <link rel=\"canonical\" href=\"...\">\n",
    "    <link rel=\"canonical\" href=\"...\">\n",
    "    \n",
    "    \n",
    "        Or canonical added via:\n",
    "        \n",
    "        HTML head\n",
    "        \n",
    "        JS injection\n",
    "        \n",
    "        HTTP header\n",
    "        \n",
    "        â†’ It breaks trust.\n",
    "        \n",
    "        analyzer does not detect this.\n",
    "\n",
    "3ï¸âƒ£ NO CHECK for CONFlicting Canonical Logic (IMPORTANT)\n",
    "\n",
    "    should detect pages where:\n",
    "    \n",
    "    Canonical points to URL A\n",
    "    \n",
    "    But redirects to URL B\n",
    "    \n",
    "    But sitemap lists URL C\n",
    "    \n",
    "    This is called canonical drift.\n",
    "    \n",
    "    Your code only checks HEAD 200 â€” not consistency.\n",
    "\n",
    "4ï¸âƒ£ NO CHECK for CANONICAL REDIRECTS (IMPORTANT)\n",
    "\n",
    "    Canonical URL should not redirect.\n",
    "    \n",
    "    Right now, you only do:\n",
    "    \n",
    "    HEAD â†’ status_code == 200\n",
    "    \n",
    "    \n",
    "    But if canonical returns:\n",
    "    \n",
    "    301\n",
    "    \n",
    "    302\n",
    "    \n",
    "    307\n",
    "    \n",
    "    You treat it as FAIL, but you do not differentiate WHY.\n",
    "    \n",
    "    You need:\n",
    "    \n",
    "    Direct 200 PASS\n",
    "    \n",
    "    3xx â†’ WARN (fix target)\n",
    "    \n",
    "    4xx/5xx â†’ FAIL\n",
    "\n",
    "5ï¸âƒ£ NO CHECK for PARAMETER CANONICALIZATION (IMPORTANT)\n",
    "\n",
    "    URLs like:\n",
    "    \n",
    "    /?ref=twitter\n",
    "    /?utm_source=x\n",
    "    /?session=abc\n",
    "    /?sort=price\n",
    "    \n",
    "    Your analyzer does not check whether canonical removes parameters.\n",
    "    \n",
    "    You should detect pages where:\n",
    "    \n",
    "    URL has params\n",
    "    \n",
    "    Canonical should strip them\n",
    "    \n",
    "    But doesnâ€™t\n",
    "\n",
    "6ï¸âƒ£ NO CHECK: Sitemap vs Canonical Alignment (ADVANCED)\n",
    "\n",
    "    You never check:\n",
    "    \n",
    "    â€œDoes the sitemap URL match the canonical target?â€\n",
    "    \n",
    "    This is a major governance error.\n",
    "    \n",
    "    If sitemap lists:\n",
    "    \n",
    "    https://example.com/blog/article\n",
    "    \n",
    "    \n",
    "    But canonical says:\n",
    "    \n",
    "    https://example.com/articles/123/\n",
    "    \n",
    "    \n",
    "    â†’ The AI agent doesn't know which is authoritative.\n",
    "    \n",
    "    You must detect this mismatch.\n",
    "\n",
    "7ï¸âƒ£ NO CHECK for WWW vs NON-WWW Canonical Conflicts (ADVANCED)\n",
    "\n",
    "    If:\n",
    "    \n",
    "    Page lives at www\n",
    "    \n",
    "    Canonical points to non-www\n",
    "    \n",
    "    Or vice-versa.\n",
    "    \n",
    "    You currently don't validate domain alignment.\n",
    "\n",
    "8ï¸âƒ£ NO CHECK for HTTP vs HTTPS Canonical Conflicts (ADVANCED)\n",
    "\n",
    "    Example conflict:\n",
    "    \n",
    "    Live page:\n",
    "    https://example.com/about\n",
    "    \n",
    "    Canonical:\n",
    "    http://example.com/about\n",
    "    \n",
    "    This is a trust degradation.\n",
    "\n",
    "9ï¸âƒ£ NO CHECK for CANONICAL CYCLES (ADVANCED)\n",
    "\n",
    "    Case:\n",
    "    \n",
    "    Page A canonical â†’ Page B\n",
    "    \n",
    "    Page B canonical â†’ Page A\n",
    "    \n",
    "    Your analyzer does not detect loops.\n",
    "\n",
    "ğŸ”Ÿ NO PLURAL CANONICAL ENFORCEMENT RULES (ADVANCED)\n",
    "\n",
    "    You should validate:\n",
    "    \n",
    "    Canonical always points to final resolved URL\n",
    "    \n",
    "    Canonical never points to:\n",
    "    â€“ paginated pages\n",
    "    â€“ filtered pages\n",
    "    â€“ search results\n",
    "    â€“ admin URLs\n",
    "    \n",
    "    Current code does none of this.\n",
    "\n",
    "1ï¸âƒ£1ï¸âƒ£ NO CONTENT SIMILARITY CHECK (OPTIONAL / AI-HEAVY)\n",
    "\n",
    "    True duplicate detection requires:\n",
    "    \n",
    "    Fetch canonical page\n",
    "    \n",
    "    Compare textual similarity\n",
    "    \n",
    "    If similarity < 85% â†’ canonical mismatch\n",
    "    \n",
    "    This is advanced but expected in ARI v10 Full Suite.\n",
    "\n",
    "\n",
    "\n",
    "1ï¸âƒ£2ï¸âƒ£ NO DETECTION OF META OR HTTP-HEADER CANONICAL (ADVANCED)\n",
    "\n",
    "    Some sites use:\n",
    "    \n",
    "    <meta name=\"canonical\"> (bad)\n",
    "    \n",
    "    Link: <url>; rel=\"canonical\" (HTTP header level)\n",
    "    \n",
    "    Your analyzer never checks response headers.\n",
    "\n",
    "\n",
    "Summary â€” Here Are the Missing Pieces\n",
    "\n",
    "| Area                  | Missing Check              | Impact   |\n",
    "| --------------------- | -------------------------- | -------- |\n",
    "| Canonical correctness | Self-reference             | CRITICAL |\n",
    "| Structural validation | Multiple canonical tags    | CRITICAL |\n",
    "| Consistency           | Sitemap â†” Canonical        | HIGH     |\n",
    "| URL hygiene           | Redirecting canonicals     | HIGH     |\n",
    "| URL integrity         | Parameter stripping        | HIGH     |\n",
    "| Protocol/Domain       | HTTPS/HTTP & WWW alignment | HIGH     |\n",
    "| Loop detection        | Canonical cycles           | MEDIUM   |\n",
    "| Header-level signals  | HTTP canonical             | MEDIUM   |\n",
    "| Content validation    | Real duplicate analysis    | OPTIONAL |\n",
    "\n",
    "\n",
    "===========TL;DR â€” Whatâ€™s Missing ===\n",
    "\n",
    "    Your analyzer checks presence, absolute, reachable.\n",
    "    \n",
    "    It DOES NOT check:\n",
    "    \n",
    "    1. correctness\n",
    "    2. consistency\n",
    "    3. conflicts\n",
    "    4. canonical drift\n",
    "    5. alignment\n",
    "    6. multi-source signals\n",
    "    7. URL normalization\n",
    "    8. real duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c3964-858a-463d-9bad-cc9d9cf7f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "----------IMPROVE VERSION ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a87e6ad1-3f6f-4c2e-bd6a-bfdb55e21161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse, urljoin, urldefrag, parse_qs\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import collections\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "# Helper functions for formatted output\n",
    "def print_header(text):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\" {text}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "def print_subheader(text):\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\" {text}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "def print_status(message, status):\n",
    "    padded_message = f\"{message:<50}\"\n",
    "    if status in [\"FAIL\", \"CRITICAL\"]:\n",
    "        status_str = f\"[\\033[91m{status}\\033[0m]\"\n",
    "    elif status == \"WARN\":\n",
    "        status_str = f\"[\\033[93m{status}\\033[0m]\"\n",
    "    elif status in [\"PASS\", \"INFO\"]:\n",
    "        status_str = f\"[\\033[92m{status}\\033[0m]\"\n",
    "    else:\n",
    "        status_str = f\"[{status}]\"\n",
    "    print(f\"{padded_message} {status_str}\")\n",
    "\n",
    "def print_recommendation(rec):\n",
    "    print(f\"  â†’ {rec}\")\n",
    "\n",
    "class EnhancedCanonicalizationAnalyzer:\n",
    "    \"\"\"\n",
    "    Enhanced analyzer for ARI v10.0 Pillar 1, Sub-pillar 3.\n",
    "    Checks canonicalization correctness, consistency, and conflicts.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, max_pages_to_check=25):\n",
    "        self.base_url = self._format_base_url(base_url)\n",
    "        self.max_pages_to_check = max_pages_to_check\n",
    "        self.urls_to_check = collections.deque()\n",
    "        self.checked_urls = set()\n",
    "        self.sitemap_urls = set()  # Track URLs from sitemap\n",
    "        \n",
    "        # Enhanced tracking\n",
    "        self.report = {\n",
    "            \"pages_checked\": 0,\n",
    "            \"pages_with_canonical\": 0,\n",
    "            \"pages_with_absolute_canonical\": 0,\n",
    "            \"pages_with_valid_canonical_target\": 0,\n",
    "            \"self_referencing_canonicals\": 0,\n",
    "            \"multiple_canonical_tags\": 0,\n",
    "            \"canonical_redirects\": 0,\n",
    "            \"http_header_canonicals\": 0,\n",
    "            \"canonical_cycles_detected\": 0,\n",
    "            \"www_mismatches\": 0,\n",
    "            \"protocol_mismatches\": 0,\n",
    "            \"parameter_issues\": 0,\n",
    "            \"sitemap_canonical_mismatches\": 0,\n",
    "            \"issues\": [],\n",
    "            \"critical_issues\": [],\n",
    "            \"warnings\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"score\": 0,\n",
    "            \"status\": \"Not Assessed\"\n",
    "        }\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'ARI-Enhanced-Canonical-Analyzer/2.0',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        })\n",
    "        \n",
    "        # Track canonical relationships for cycle detection\n",
    "        self.canonical_graph = {}\n",
    "\n",
    "    def _format_base_url(self, url):\n",
    "        \"\"\"Normalize base URL to include scheme.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        if not parsed.scheme:\n",
    "            url = \"https://\" + url\n",
    "            parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "    def _normalize_url(self, url):\n",
    "        \"\"\"\n",
    "        Normalize URL by:\n",
    "        - Removing fragment (#anchor)\n",
    "        - Converting to lowercase for domain\n",
    "        - Removing trailing slash for consistency (except root)\n",
    "        \"\"\"\n",
    "        clean_url = urldefrag(url).url\n",
    "        parsed = urlparse(clean_url)\n",
    "        \n",
    "        # Normalize domain to lowercase\n",
    "        netloc = parsed.netloc.lower()\n",
    "        \n",
    "        # Remove trailing slash except for root path\n",
    "        path = parsed.path\n",
    "        if path != '/' and path.endswith('/'):\n",
    "            path = path.rstrip('/')\n",
    "        \n",
    "        normalized = f\"{parsed.scheme}://{netloc}{path}\"\n",
    "        if parsed.query:\n",
    "            normalized += f\"?{parsed.query}\"\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "    def _fetch_url(self, url, method='GET', allow_redirects=True):\n",
    "        \"\"\"Fetch URL with error handling.\"\"\"\n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = self.session.get(url, timeout=10, allow_redirects=allow_redirects)\n",
    "            elif method == 'HEAD':\n",
    "                response = self.session.head(url, timeout=10, allow_redirects=allow_redirects)\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.report[\"issues\"].append(f\"Network error for {url}: {str(e)[:100]}\")\n",
    "            return None\n",
    "\n",
    "    def _check_http_header_canonical(self, response) -> Optional[str]:\n",
    "        \"\"\"Check for canonical URL in HTTP Link header.\"\"\"\n",
    "        link_header = response.headers.get('Link', '')\n",
    "        if 'rel=\"canonical\"' in link_header or \"rel='canonical'\" in link_header:\n",
    "            # Parse Link header (simplified parsing)\n",
    "            parts = link_header.split(',')\n",
    "            for part in parts:\n",
    "                if 'canonical' in part:\n",
    "                    url_part = part.split(';')[0].strip()\n",
    "                    canonical_url = url_part.strip('<>')\n",
    "                    return canonical_url\n",
    "        return None\n",
    "\n",
    "    def get_urls_from_sitemap(self):\n",
    "        \"\"\"Fetch URLs from sitemap.xml.\"\"\"\n",
    "        print_status(\"Fetching URLs from sitemap\", \"INFO\")\n",
    "        sitemap_url = urljoin(self.base_url, 'sitemap.xml')\n",
    "        response = self._fetch_url(sitemap_url)\n",
    "        \n",
    "        if not response or response.status_code != 200:\n",
    "            print_status(\"Could not fetch sitemap.xml\", \"WARN\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            content = response.content\n",
    "            if response.url.endswith('.gz') or 'gzip' in response.headers.get('Content-Type', ''):\n",
    "                content = gzip.decompress(content)\n",
    "\n",
    "            root = ET.fromstring(content)\n",
    "            namespace = root.tag.split('}')[0][1:] if '}' in root.tag else ''\n",
    "\n",
    "            for url_node in root.findall(f'.//{{{namespace}}}loc'):\n",
    "                url = url_node.text.strip()\n",
    "                self.sitemap_urls.add(self._normalize_url(url))\n",
    "                if len(self.urls_to_check) < self.max_pages_to_check:\n",
    "                    self.urls_to_check.append(url)\n",
    "                    \n",
    "            print_status(f\"Found {len(self.sitemap_urls)} URLs in sitemap\", \"PASS\")\n",
    "        except Exception as e:\n",
    "            self.report[\"warnings\"].append(f\"Sitemap parsing error: {str(e)[:100]}\")\n",
    "            print_status(\"Sitemap parsing failed\", \"WARN\")\n",
    "\n",
    "    def _has_tracking_parameters(self, url):\n",
    "        \"\"\"Check if URL has tracking/session parameters.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        if not parsed.query:\n",
    "            return False\n",
    "        \n",
    "        params = parse_qs(parsed.query)\n",
    "        tracking_params = ['utm_source', 'utm_medium', 'utm_campaign', 'ref', \n",
    "                          'source', 'session', 'sid', 'fbclid', 'gclid']\n",
    "        \n",
    "        return any(param in params for param in tracking_params)\n",
    "\n",
    "    def analyze_page(self, url):\n",
    "        \"\"\"Comprehensive analysis of a single page's canonicalization.\"\"\"\n",
    "        print(f\"\\nâ†’ Analyzing: {url}\")\n",
    "        self.report[\"pages_checked\"] += 1\n",
    "        \n",
    "        # Fetch the page\n",
    "        response = self._fetch_url(url, 'GET')\n",
    "        if not response or 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "            print_status(\"Not valid HTML or unreachable\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        clean_current_url = self._normalize_url(response.url)\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 1: Multiple Canonical Tags (CRITICAL)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        canonical_tags = soup.find_all('link', {'rel': 'canonical'})\n",
    "        \n",
    "        if len(canonical_tags) > 1:\n",
    "            self.report[\"multiple_canonical_tags\"] += 1\n",
    "            self.report[\"critical_issues\"].append(\n",
    "                f\"CRITICAL: Multiple canonical tags ({len(canonical_tags)}) on {url}\"\n",
    "            )\n",
    "            print_status(\"Multiple canonical tags detected\", \"CRITICAL\")\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 2: HTTP Header Canonical (ADVANCED)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        http_canonical = self._check_http_header_canonical(response)\n",
    "        if http_canonical:\n",
    "            self.report[\"http_header_canonicals\"] += 1\n",
    "            print_status(\"HTTP Link header canonical found\", \"INFO\")\n",
    "        \n",
    "        # Get primary canonical from HTML\n",
    "        canonical_tag = canonical_tags[0] if canonical_tags else None\n",
    "        \n",
    "        if not canonical_tag:\n",
    "            self.report[\"critical_issues\"].append(f\"Missing canonical tag: {url}\")\n",
    "            print_status(\"Canonical tag presence\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        self.report[\"pages_with_canonical\"] += 1\n",
    "        print_status(\"Canonical tag presence\", \"PASS\")\n",
    "\n",
    "        href = canonical_tag.get('href', '').strip()\n",
    "        if not href:\n",
    "            self.report[\"critical_issues\"].append(f\"Empty canonical href on: {url}\")\n",
    "            print_status(\"Canonical href validity\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 3: Absolute URL Check\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        parsed_href = urlparse(href)\n",
    "        if not (parsed_href.scheme and parsed_href.netloc):\n",
    "            self.report[\"critical_issues\"].append(\n",
    "                f\"Relative canonical '{href}' on: {url}\"\n",
    "            )\n",
    "            print_status(\"Canonical is absolute\", \"FAIL\")\n",
    "            return\n",
    "\n",
    "        self.report[\"pages_with_absolute_canonical\"] += 1\n",
    "        print_status(\"Canonical is absolute\", \"PASS\")\n",
    "\n",
    "        clean_canonical_url = self._normalize_url(href)\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 4: Self-Referencing Canonical (CRITICAL)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if clean_current_url == clean_canonical_url:\n",
    "            self.report[\"self_referencing_canonicals\"] += 1\n",
    "            print_status(\"Self-referencing canonical\", \"PASS\")\n",
    "        else:\n",
    "            # Check if this is a legitimate non-self-reference\n",
    "            # (pagination, filters, variants should point elsewhere)\n",
    "            is_paginated = any(p in url.lower() for p in ['page=', '/page/', '?p='])\n",
    "            has_filters = any(f in url.lower() for f in ['filter=', 'sort=', 'category='])\n",
    "            \n",
    "            if not (is_paginated or has_filters):\n",
    "                self.report[\"warnings\"].append(\n",
    "                    f\"Non-self-referencing canonical: {url} â†’ {clean_canonical_url}\"\n",
    "                )\n",
    "                print_status(\"Non-self-referencing (check if valid)\", \"WARN\")\n",
    "            else:\n",
    "                print_status(\"Points to canonical version (paginated/filtered)\", \"INFO\")\n",
    "\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 5: WWW vs Non-WWW Alignment (ADVANCED)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        current_domain = urlparse(clean_current_url).netloc\n",
    "        canonical_domain = urlparse(clean_canonical_url).netloc\n",
    "        \n",
    "        if current_domain.replace('www.', '') != canonical_domain.replace('www.', ''):\n",
    "            if current_domain != canonical_domain:\n",
    "                self.report[\"www_mismatches\"] += 1\n",
    "                self.report[\"warnings\"].append(\n",
    "                    f\"Domain mismatch: {current_domain} â†’ {canonical_domain}\"\n",
    "                )\n",
    "                print_status(\"WWW/domain consistency\", \"WARN\")\n",
    "\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 6: HTTP vs HTTPS Protocol (ADVANCED)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        current_protocol = urlparse(clean_current_url).scheme\n",
    "        canonical_protocol = urlparse(clean_canonical_url).scheme\n",
    "        \n",
    "        if current_protocol != canonical_protocol:\n",
    "            self.report[\"protocol_mismatches\"] += 1\n",
    "            self.report[\"critical_issues\"].append(\n",
    "                f\"Protocol mismatch: {current_protocol} â†’ {canonical_protocol} on {url}\"\n",
    "            )\n",
    "            print_status(\"Protocol consistency (HTTP/HTTPS)\", \"CRITICAL\")\n",
    "\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 7: Parameter Canonicalization (IMPORTANT)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if self._has_tracking_parameters(url):\n",
    "            canonical_has_params = bool(urlparse(clean_canonical_url).query)\n",
    "            if canonical_has_params:\n",
    "                self.report[\"parameter_issues\"] += 1\n",
    "                self.report[\"warnings\"].append(\n",
    "                    f\"Tracking params not stripped in canonical: {url}\"\n",
    "                )\n",
    "                print_status(\"Parameter stripping\", \"WARN\")\n",
    "            else:\n",
    "                print_status(\"Tracking parameters properly removed\", \"PASS\")\n",
    "\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 8: Canonical Target Accessibility & Redirects (IMPORTANT)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        head_response = self._fetch_url(clean_canonical_url, 'HEAD', allow_redirects=False)\n",
    "        \n",
    "        if head_response:\n",
    "            status = head_response.status_code\n",
    "            \n",
    "            if status == 200:\n",
    "                self.report[\"pages_with_valid_canonical_target\"] += 1\n",
    "                print_status(\"Canonical target accessible\", \"PASS\")\n",
    "            elif 300 <= status < 400:\n",
    "                self.report[\"canonical_redirects\"] += 1\n",
    "                redirect_location = head_response.headers.get('Location', 'unknown')\n",
    "                self.report[\"warnings\"].append(\n",
    "                    f\"Canonical redirects ({status}): {clean_canonical_url} â†’ {redirect_location}\"\n",
    "                )\n",
    "                print_status(f\"Canonical redirects ({status})\", \"WARN\")\n",
    "            elif 400 <= status < 500:\n",
    "                self.report[\"critical_issues\"].append(\n",
    "                    f\"Canonical returns {status} (client error): {clean_canonical_url}\"\n",
    "                )\n",
    "                print_status(f\"Canonical target broken ({status})\", \"FAIL\")\n",
    "            else:\n",
    "                self.report[\"critical_issues\"].append(\n",
    "                    f\"Canonical returns {status} (server error): {clean_canonical_url}\"\n",
    "                )\n",
    "                print_status(f\"Canonical target error ({status})\", \"FAIL\")\n",
    "        else:\n",
    "            print_status(\"Cannot verify canonical target\", \"WARN\")\n",
    "\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 9: Sitemap vs Canonical Alignment (ADVANCED)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if self.sitemap_urls:\n",
    "            normalized_sitemap_url = self._normalize_url(url)\n",
    "            if normalized_sitemap_url in self.sitemap_urls:\n",
    "                if clean_canonical_url != normalized_sitemap_url:\n",
    "                    self.report[\"sitemap_canonical_mismatches\"] += 1\n",
    "                    self.report[\"warnings\"].append(\n",
    "                        f\"Sitemap/canonical mismatch: sitemap has {url}, canonical points to {clean_canonical_url}\"\n",
    "                    )\n",
    "                    print_status(\"Sitemap-canonical alignment\", \"WARN\")\n",
    "\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # CHECK 10: Track Canonical Graph for Cycle Detection\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        self.canonical_graph[clean_current_url] = clean_canonical_url\n",
    "\n",
    "    def _detect_canonical_cycles(self):\n",
    "        \"\"\"Detect canonical cycles after all pages analyzed.\"\"\"\n",
    "        print_subheader(\"Detecting Canonical Cycles\")\n",
    "        \n",
    "        for start_url, canonical_url in self.canonical_graph.items():\n",
    "            visited = set()\n",
    "            current = start_url\n",
    "            \n",
    "            while current in self.canonical_graph:\n",
    "                if current in visited:\n",
    "                    # Cycle detected\n",
    "                    self.report[\"canonical_cycles_detected\"] += 1\n",
    "                    cycle_path = \" â†’ \".join(list(visited) + [current])\n",
    "                    self.report[\"critical_issues\"].append(\n",
    "                        f\"CANONICAL CYCLE: {cycle_path}\"\n",
    "                    )\n",
    "                    print_status(f\"Cycle detected starting at {start_url}\", \"CRITICAL\")\n",
    "                    break\n",
    "                \n",
    "                visited.add(current)\n",
    "                next_url = self.canonical_graph.get(current)\n",
    "                \n",
    "                if not next_url or next_url == current:\n",
    "                    break\n",
    "                    \n",
    "                current = next_url\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Main execution logic.\"\"\"\n",
    "        print_header(\"ARI v10.0 Enhanced Canonicalization Analyzer\")\n",
    "        print(\"Pillar 1, Sub-pillar 3: Canonicalization & Source Singularity\\n\")\n",
    "        \n",
    "        # Fetch sitemap\n",
    "        self.get_urls_from_sitemap()\n",
    "        \n",
    "        # Fallback to homepage if no sitemap\n",
    "        if not self.urls_to_check:\n",
    "            self.urls_to_check.append(self.base_url)\n",
    "\n",
    "        print_subheader(f\"Analyzing up to {self.max_pages_to_check} pages\")\n",
    "        \n",
    "        # Analyze pages\n",
    "        while self.urls_to_check and len(self.checked_urls) < self.max_pages_to_check:\n",
    "            url = self.urls_to_check.popleft()\n",
    "            if url in self.checked_urls:\n",
    "                continue\n",
    "            \n",
    "            self.checked_urls.add(url)\n",
    "            self.analyze_page(url)\n",
    "            time.sleep(0.2)  # Rate limiting\n",
    "\n",
    "        # Post-analysis checks\n",
    "        self._detect_canonical_cycles()\n",
    "        self._generate_final_report()\n",
    "        self._print_final_report()\n",
    "\n",
    "    def _generate_final_report(self):\n",
    "        \"\"\"Calculate final score and generate recommendations.\"\"\"\n",
    "        if self.report[\"pages_checked\"] == 0:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 0\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Could not analyze any pages. Check URL accessibility.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Scoring weights (total 100 points)\n",
    "        checked = self.report[\"pages_checked\"]\n",
    "        \n",
    "        # Base metrics\n",
    "        p_canonical = self.report[\"pages_with_canonical\"] / checked\n",
    "        p_absolute = self.report[\"pages_with_absolute_canonical\"] / checked\n",
    "        p_valid = self.report[\"pages_with_valid_canonical_target\"] / checked\n",
    "        p_self_ref = self.report[\"self_referencing_canonicals\"] / checked if checked > 0 else 0\n",
    "        \n",
    "        # Penalty metrics\n",
    "        critical_penalty = min(30, self.report[\"multiple_canonical_tags\"] * 10 + \n",
    "                              len(self.report[\"critical_issues\"]) * 2)\n",
    "        warning_penalty = min(20, len(self.report[\"warnings\"]) * 1)\n",
    "        \n",
    "        # Calculate score\n",
    "        base_score = (p_canonical * 30 + p_absolute * 20 + p_valid * 20 + p_self_ref * 30)\n",
    "        final_score = max(0, base_score - critical_penalty - warning_penalty)\n",
    "        \n",
    "        self.report[\"score\"] = int(final_score)\n",
    "        \n",
    "        # Status determination\n",
    "        if final_score >= 90:\n",
    "            self.report[\"status\"] = \"Excellent\"\n",
    "        elif final_score >= 75:\n",
    "            self.report[\"status\"] = \"Good\"\n",
    "        elif final_score >= 50:\n",
    "            self.report[\"status\"] = \"Needs Improvement\"\n",
    "        else:\n",
    "            self.report[\"status\"] = \"Poor\"\n",
    "\n",
    "        # Generate recommendations\n",
    "        if p_canonical < 1.0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Add <link rel='canonical'> tags to all indexable pages\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"multiple_canonical_tags\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"CRITICAL: Remove duplicate canonical tags - only one per page\"\n",
    "            )\n",
    "        \n",
    "        if p_self_ref < 0.8:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Ensure canonical tags self-reference unless page is paginated/filtered\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"protocol_mismatches\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Fix HTTP/HTTPS protocol mismatches in canonical URLs\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"canonical_redirects\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Update canonical URLs to point directly to final destination (avoid redirects)\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"sitemap_canonical_mismatches\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Align sitemap URLs with canonical targets for consistency\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"canonical_cycles_detected\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"CRITICAL: Break canonical cycles - pages pointing to each other\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"parameter_issues\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Strip tracking/session parameters in canonical URLs\"\n",
    "            )\n",
    "\n",
    "    def _print_final_report(self):\n",
    "        \"\"\"Print comprehensive final report.\"\"\"\n",
    "        print_header(\"Final Assessment Report\")\n",
    "        \n",
    "        # Overall status\n",
    "        print_status(\"Overall Status\", self.report['status'])\n",
    "        print_status(f\"ARI Score (out of 100)\", str(self.report['score']))\n",
    "        \n",
    "        # Summary statistics\n",
    "        print_subheader(\"Analysis Summary\")\n",
    "        print(f\"  Pages Analyzed: {self.report['pages_checked']}\")\n",
    "        print(f\"  Pages with Canonical: {self.report['pages_with_canonical']}\")\n",
    "        print(f\"  Self-Referencing: {self.report['self_referencing_canonicals']}\")\n",
    "        print(f\"  Valid Targets: {self.report['pages_with_valid_canonical_target']}\")\n",
    "        \n",
    "        # Issues summary\n",
    "        print_subheader(\"Issues Detected\")\n",
    "        print(f\"  Critical Issues: {len(self.report['critical_issues'])}\")\n",
    "        print(f\"  Warnings: {len(self.report['warnings'])}\")\n",
    "        print(f\"  Multiple Canonical Tags: {self.report['multiple_canonical_tags']}\")\n",
    "        print(f\"  Canonical Cycles: {self.report['canonical_cycles_detected']}\")\n",
    "        print(f\"  Canonical Redirects: {self.report['canonical_redirects']}\")\n",
    "        print(f\"  Protocol Mismatches: {self.report['protocol_mismatches']}\")\n",
    "        print(f\"  Sitemap Mismatches: {self.report['sitemap_canonical_mismatches']}\")\n",
    "        \n",
    "        # Critical issues details\n",
    "        if self.report[\"critical_issues\"]:\n",
    "            print_subheader(\"Critical Issues (Top 5)\")\n",
    "            for issue in self.report[\"critical_issues\"][:5]:\n",
    "                print(f\"  âš ï¸  {issue}\")\n",
    "        \n",
    "        # Warnings details\n",
    "        if self.report[\"warnings\"]:\n",
    "            print_subheader(\"Warnings (Top 5)\")\n",
    "            for warning in self.report[\"warnings\"][:5]:\n",
    "                print(f\"  âš¡ {warning}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        if self.report[\"recommendations\"]:\n",
    "            print_subheader(\"Actionable Recommendations\")\n",
    "            for i, rec in enumerate(self.report[\"recommendations\"], 1):\n",
    "                print_recommendation(f\"{i}. {rec}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac7bb820-da38-4848-8c6e-e0af2f2d33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedCanonicalizationAnalyzer============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:   https://vercel.com/docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " ARI v10.0 Enhanced Canonicalization Analyzer\n",
      "======================================================================\n",
      "Pillar 1, Sub-pillar 3: Canonicalization & Source Singularity\n",
      "\n",
      "Fetching URLs from sitemap                         [\u001b[92mINFO\u001b[0m]\n",
      "Found 3803 URLs in sitemap                         [\u001b[92mPASS\u001b[0m]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Analyzing up to 25 pages\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/\n",
      "Canonical tag presence                             [\u001b[92mPASS\u001b[0m]\n",
      "Canonical is absolute                              [\u001b[92mPASS\u001b[0m]\n",
      "Non-self-referencing (check if valid)              [\u001b[93mWARN\u001b[0m]\n",
      "Canonical target accessible                        [\u001b[92mPASS\u001b[0m]\n",
      "Sitemap-canonical alignment                        [\u001b[93mWARN\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/about\n",
      "Canonical tag presence                             [\u001b[92mPASS\u001b[0m]\n",
      "Canonical is absolute                              [\u001b[92mPASS\u001b[0m]\n",
      "Self-referencing canonical                         [\u001b[92mPASS\u001b[0m]\n",
      "Canonical target accessible                        [\u001b[92mPASS\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/abuse\n",
      "Canonical tag presence                             [\u001b[92mPASS\u001b[0m]\n",
      "Canonical is absolute                              [\u001b[92mPASS\u001b[0m]\n",
      "Self-referencing canonical                         [\u001b[92mPASS\u001b[0m]\n",
      "Canonical target accessible                        [\u001b[92mPASS\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/ai-elements\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/ai-sdk-dev-setup\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/automatic-summarization\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/basic-chatbot\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/conclusion\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/data-extraction\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/introduction-to-invisible-ai\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/introduction-to-llms\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/model-types-and-performance\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/multi-step-and-generative-ui\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/prompting-fundamentals\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/structured-data-extraction\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/system-prompts\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/text-classification\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/tool-use\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/ai-sdk/ui-with-v0\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/shadcn-ui\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/shadcn-ui/adding-your-first-component\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/shadcn-ui/anatomy-of-a-primitive\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "â†’ Analyzing: https://vercel.com/academy/shadcn-ui/components-json\n",
      "Canonical tag presence                             [\u001b[91mFAIL\u001b[0m]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Detecting Canonical Cycles\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      " Final Assessment Report\n",
      "======================================================================\n",
      "Overall Status                                     [Poor]\n",
      "ARI Score (out of 100)                             [0]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Analysis Summary\n",
      "----------------------------------------------------------------------\n",
      "  Pages Analyzed: 25\n",
      "  Pages with Canonical: 3\n",
      "  Self-Referencing: 2\n",
      "  Valid Targets: 3\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Issues Detected\n",
      "----------------------------------------------------------------------\n",
      "  Critical Issues: 22\n",
      "  Warnings: 2\n",
      "  Multiple Canonical Tags: 0\n",
      "  Canonical Cycles: 0\n",
      "  Canonical Redirects: 0\n",
      "  Protocol Mismatches: 0\n",
      "  Sitemap Mismatches: 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Critical Issues (Top 5)\n",
      "----------------------------------------------------------------------\n",
      "  âš ï¸  Missing canonical tag: https://vercel.com/academy\n",
      "  âš ï¸  Missing canonical tag: https://vercel.com/academy/ai-sdk\n",
      "  âš ï¸  Missing canonical tag: https://vercel.com/academy/ai-sdk/ai-elements\n",
      "  âš ï¸  Missing canonical tag: https://vercel.com/academy/ai-sdk/ai-sdk-dev-setup\n",
      "  âš ï¸  Missing canonical tag: https://vercel.com/academy/ai-sdk/automatic-summarization\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Warnings (Top 5)\n",
      "----------------------------------------------------------------------\n",
      "  âš¡ Non-self-referencing canonical: https://vercel.com/ â†’ https://vercel.com\n",
      "  âš¡ Sitemap/canonical mismatch: sitemap has https://vercel.com/, canonical points to https://vercel.com\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Actionable Recommendations\n",
      "----------------------------------------------------------------------\n",
      "  â†’ 1. Add <link rel='canonical'> tags to all indexable pages\n",
      "  â†’ 2. Ensure canonical tags self-reference unless page is paginated/filtered\n",
      "  â†’ 3. Align sitemap URLs with canonical targets for consistency\n",
      "\n",
      "============================================================\n",
      "RAW REPORT OBJECT\n",
      "============================================================\n",
      "{'pages_checked': 25, 'pages_with_canonical': 3, 'pages_with_absolute_canonical': 3, 'pages_with_valid_canonical_target': 3, 'self_referencing_canonicals': 2, 'multiple_canonical_tags': 0, 'canonical_redirects': 0, 'http_header_canonicals': 0, 'canonical_cycles_detected': 0, 'www_mismatches': 0, 'protocol_mismatches': 0, 'parameter_issues': 0, 'sitemap_canonical_mismatches': 1, 'issues': [], 'critical_issues': ['Missing canonical tag: https://vercel.com/academy', 'Missing canonical tag: https://vercel.com/academy/ai-sdk', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/ai-elements', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/ai-sdk-dev-setup', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/automatic-summarization', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/basic-chatbot', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/conclusion', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/data-extraction', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/introduction-to-invisible-ai', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/introduction-to-llms', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/model-types-and-performance', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/multi-step-and-generative-ui', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/prompting-fundamentals', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/structured-data-extraction', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/system-prompts', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/text-classification', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/tool-use', 'Missing canonical tag: https://vercel.com/academy/ai-sdk/ui-with-v0', 'Missing canonical tag: https://vercel.com/academy/shadcn-ui', 'Missing canonical tag: https://vercel.com/academy/shadcn-ui/adding-your-first-component', 'Missing canonical tag: https://vercel.com/academy/shadcn-ui/anatomy-of-a-primitive', 'Missing canonical tag: https://vercel.com/academy/shadcn-ui/components-json'], 'warnings': ['Non-self-referencing canonical: https://vercel.com/ â†’ https://vercel.com', 'Sitemap/canonical mismatch: sitemap has https://vercel.com/, canonical points to https://vercel.com'], 'recommendations': [\"Add <link rel='canonical'> tags to all indexable pages\", 'Ensure canonical tags self-reference unless page is paginated/filtered', 'Align sitemap URLs with canonical targets for consistency'], 'score': 0, 'status': 'Poor'}\n"
     ]
    }
   ],
   "source": [
    "# Simple header\n",
    "print(\"EnhancedCanonicalizationAnalyzer\" + \"=\"*60)\n",
    "\n",
    "# Get URL from user\n",
    "target_url = input(\"Enter URL: \").strip()\n",
    "\n",
    "# Instantiate the analyzer with your URL\n",
    "analyzer = EnhancedCanonicalizationAnalyzer(base_url=target_url, max_pages_to_check=25)\n",
    "\n",
    "# Run the analysis\n",
    "analyzer.run_analysis()\n",
    "\n",
    "# Print structured raw report for dev / API use\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW REPORT OBJECT\")\n",
    "print(\"=\"*60)\n",
    "print(analyzer.report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588bc60-d566-4421-823e-05aa77881ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
