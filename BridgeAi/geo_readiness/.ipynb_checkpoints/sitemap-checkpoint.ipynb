{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbede2-ef76-459f-acb7-2836e26107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------Sitemap Health & Freshness ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614a8f9-d972-4053-b225-64ebc2249445",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyzes a website's sitemap health and freshness based on ARI v10.0 Pillar 1, Sub-pillar 1.\n",
    "Purpose:\n",
    "    Analyzes a website's sitemap health and freshness to ensure AI agents can efficiently discover all content.\n",
    "\n",
    "Key Checks:\n",
    "\n",
    "    1. Presence of sitemap(s) and correct XML format.\n",
    "\n",
    "    2. Accurate URLs reflecting current site structure.\n",
    "\n",
    "    3. Timely updates to reflect new, modified, or removed pages.\n",
    "\n",
    "    4. Validation against schema standards to prevent parsing errors.\n",
    "\n",
    "    5. Detect broken links or orphaned pages that may reduce crawl efficiency.\n",
    "\n",
    "Outcome:\n",
    "    A healthy, fresh sitemap ensures maximum discoverability and trustworthiness for AI agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505b3d83-bdd3-44cb-829c-563e4290543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pillar 1, Sub-pillar 1\n",
    "# See Bridge.ipynb cell 1 for logic\n",
    "# ...existing code...\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Helper to print colored and formatted text for better readability\n",
    "def print_header(text):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def print_subheader(text):\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\" {text}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "def print_status(message, status):\n",
    "    print(f\"{message:<45} [{status}]\")\n",
    "\n",
    "def print_recommendation(rec):\n",
    "    print(f\"  - {rec}\")\n",
    "\n",
    "class SitemapAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes a website's sitemap health and freshness based on ARI v10.0 Pillar 1, Sub-pillar 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = self._format_base_url(base_url)\n",
    "        self.sitemaps_to_process = []\n",
    "        self.processed_sitemaps = set()\n",
    "        self.report = {\n",
    "            \"sitemap_locations\": [],\n",
    "            \"total_urls\": 0,\n",
    "            \"urls_with_lastmod\": 0,\n",
    "            \"error_log\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"score\": 0,\n",
    "            \"status\": \"Critical Failure\"\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'ARI-Sitemap-Analyzer/1.0'\n",
    "        })\n",
    "\n",
    "    def _format_base_url(self, url):\n",
    "        \"\"\"Ensures the URL has a scheme and is just the base domain.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        if not parsed.scheme:\n",
    "            url = \"https://\" + url\n",
    "            parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "    def _fetch_url(self, url):\n",
    "        \"\"\"Fetches a URL, handles redirects and exceptions.\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.report[\"error_log\"].append(f\"Failed to fetch {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_sitemap_content(self, response):\n",
    "        \"\"\"Decompresses content if gzipped, returns text.\"\"\"\n",
    "        if response.url.endswith('.gz') or response.headers.get('Content-Type') == 'application/gzip':\n",
    "            try:\n",
    "                with gzip.GzipFile(fileobj=BytesIO(response.content)) as gz_file:\n",
    "                    return gz_file.read().decode('utf-8')\n",
    "            except Exception as e:\n",
    "                self.report[\"error_log\"].append(f\"Failed to decompress gzipped sitemap {response.url}: {e}\")\n",
    "                return None\n",
    "        return response.text\n",
    "\n",
    "    def find_sitemaps_from_robots(self):\n",
    "        \"\"\"Parses robots.txt to find sitemap locations.\"\"\"\n",
    "        print_status(f\"Checking robots.txt at {urljoin(self.base_url, 'robots.txt')}\", \"IN PROGRESS\")\n",
    "        robots_url = urljoin(self.base_url, 'robots.txt')\n",
    "        response = self._fetch_url(robots_url)\n",
    "\n",
    "        if response:\n",
    "            lines = response.text.splitlines()\n",
    "            found = False\n",
    "            for line in lines:\n",
    "                if line.lower().startswith('sitemap:'):\n",
    "                    sitemap_url = line.split(':', 1)[1].strip()\n",
    "                    self.sitemaps_to_process.append(sitemap_url)\n",
    "                    found = True\n",
    "            if found:\n",
    "                 print_status(f\"Found {len(self.sitemaps_to_process)} sitemap(s) in robots.txt\", \"OK\")\n",
    "            else:\n",
    "                 print_status(\"No sitemap directive in robots.txt\", \"WARNING\")\n",
    "        else:\n",
    "             print_status(\"Could not fetch robots.txt\", \"WARNING\")\n",
    "\n",
    "    def _parse_sitemap(self, xml_content, sitemap_url):\n",
    "        \"\"\"Parses XML content to find URLs or other sitemaps.\"\"\"\n",
    "        try:\n",
    "            root = ET.fromstring(xml_content.encode('utf-8'))\n",
    "            namespace = root.tag.split('}')[0][1:] if '}' in root.tag else ''\n",
    "\n",
    "            # It's a sitemap index file\n",
    "            if root.tag.endswith('sitemapindex'):\n",
    "                sitemaps = root.findall(f'{{{namespace}}}sitemap')\n",
    "                for sitemap in sitemaps:\n",
    "                    loc = sitemap.find(f'{{{namespace}}}loc')\n",
    "                    if loc is not None:\n",
    "                        new_sitemap_url = loc.text.strip()\n",
    "                        if new_sitemap_url not in self.processed_sitemaps:\n",
    "                            self.sitemaps_to_process.append(new_sitemap_url)\n",
    "                print_status(f\"Parsed sitemap index: Found {len(sitemaps)} more sitemaps\", \"INFO\")\n",
    "\n",
    "            # It's a URL set\n",
    "            elif root.tag.endswith('urlset'):\n",
    "                urls = root.findall(f'{{{namespace}}}url')\n",
    "                self.report[\"total_urls\"] += len(urls)\n",
    "                for url in urls:\n",
    "                    lastmod = url.find(f'{{{namespace}}}lastmod')\n",
    "                    if lastmod is not None and lastmod.text:\n",
    "                        self.report[\"urls_with_lastmod\"] += 1\n",
    "                print_status(f\"Parsed URL set: Found {len(urls)} URLs\", \"INFO\")\n",
    "\n",
    "            else:\n",
    "                 self.report[\"error_log\"].append(f\"Unknown root tag '{root.tag}' in {sitemap_url}\")\n",
    "\n",
    "        except ET.ParseError as e:\n",
    "            self.report[\"error_log\"].append(f\"XML Parse Error in {sitemap_url}: {e}\")\n",
    "\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Main execution logic.\"\"\"\n",
    "        print_header(\"ARI Sub-Pillar 1.1: Sitemap Health & Freshness\")\n",
    "\n",
    "        # 1. Discover sitemaps\n",
    "        self.find_sitemaps_from_robots()\n",
    "        if not self.sitemaps_to_process:\n",
    "            print_status(\"Falling back to default sitemap.xml location\", \"INFO\")\n",
    "            self.sitemaps_to_process.append(urljoin(self.base_url, 'sitemap.xml'))\n",
    "\n",
    "        # 2. Process all found sitemaps (including those discovered recursively)\n",
    "        print_subheader(\"Processing Sitemaps\")\n",
    "        while self.sitemaps_to_process:\n",
    "            sitemap_url = self.sitemaps_to_process.pop(0)\n",
    "            if sitemap_url in self.processed_sitemaps:\n",
    "                continue\n",
    "\n",
    "            self.processed_sitemaps.add(sitemap_url)\n",
    "            print(f\"\\n-> Fetching: {sitemap_url}\")\n",
    "            response = self._fetch_url(sitemap_url)\n",
    "\n",
    "            if response:\n",
    "                self.report[\"sitemap_locations\"].append(sitemap_url)\n",
    "                xml_content = self._get_sitemap_content(response)\n",
    "                if xml_content:\n",
    "                    self._parse_sitemap(xml_content, response.url)\n",
    "                else:\n",
    "                    print_status(\"Failed to get sitemap content\", \"ERROR\")\n",
    "            else:\n",
    "                print_status(f\"Sitemap not found or inaccessible at {sitemap_url}\", \"ERROR\")\n",
    "\n",
    "        # 3. Generate Score and Recommendations\n",
    "        self._generate_final_report()\n",
    "\n",
    "        # 4. Print Report\n",
    "        self._print_final_report()\n",
    "\n",
    "    def _generate_final_report(self):\n",
    "        \"\"\"Calculate final score and populate recommendations.\"\"\"\n",
    "        # Critical Failures (BLOCKER)\n",
    "        if not self.report[\"sitemap_locations\"]:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 0\n",
    "            self.report[\"recommendations\"].append(\"Generate sitemap.xml using automated crawling tools as no sitemap was found.\")\n",
    "            self.report[\"recommendations\"].append(\"Submit the sitemap to Google Search Console and Bing Webmaster Tools.\")\n",
    "            return\n",
    "\n",
    "        if self.report[\"error_log\"] and any(\"XML Parse Error\" in e for e in self.report[\"error_log\"]):\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 10\n",
    "            self.report[\"recommendations\"].append(\"Sitemap is malformed. Validate XML structure and correct parsing errors.\")\n",
    "\n",
    "        if self.report[\"total_urls\"] == 0:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 15\n",
    "            self.report[\"recommendations\"].append(\"Sitemap is empty or could not be parsed correctly. Ensure it contains URL entries.\")\n",
    "            return\n",
    "\n",
    "        # Scoring Logic\n",
    "        # Base score for having a valid sitemap\n",
    "        score = 50\n",
    "\n",
    "        lastmod_percentage = self.report[\"urls_with_lastmod\"] / self.report[\"total_urls\"]\n",
    "        score += 50 * lastmod_percentage # Up to 50 points for lastmod coverage\n",
    "\n",
    "        self.report[\"score\"] = int(score)\n",
    "\n",
    "        if self.report[\"score\"] >= 95:\n",
    "            self.report[\"status\"] = \"Excellent\"\n",
    "        elif self.report[\"score\"] >= 70:\n",
    "            self.report[\"status\"] = \"Good\"\n",
    "        elif self.report[\"score\"] >= 40:\n",
    "            self.report[\"status\"] = \"Needs Improvement\"\n",
    "        else:\n",
    "            self.report[\"status\"] = \"Poor\"\n",
    "\n",
    "        # Recommendations for low scores\n",
    "        if lastmod_percentage < 0.9:\n",
    "            self.report[\"recommendations\"].append(\"Improve coverage of <lastmod> timestamps for all URLs to signal content freshness.\")\n",
    "        if lastmod_percentage < 1.0:\n",
    "            self.report[\"recommendations\"].append(\"Consider implementing automatic sitemap updates in a CI/CD pipeline to keep timestamps current.\")\n",
    "\n",
    "    def _print_final_report(self):\n",
    "        \"\"\"Prints the final formatted report.\"\"\"\n",
    "        print_header(\"Final Assessment Report\")\n",
    "        print_status(\"Overall Status\", self.report['status'])\n",
    "        print_status(\"ARI Score (out of 100)\", self.report['score'])\n",
    "\n",
    "        print_subheader(\"Summary\")\n",
    "        print(f\"Discovered and processed {len(self.report['sitemap_locations'])} sitemap file(s).\")\n",
    "        print(f\"Found a total of {self.report['total_urls']} URLs.\")\n",
    "        if self.report['total_urls'] > 0:\n",
    "            lastmod_percent = (self.report['urls_with_lastmod'] / self.report['total_urls']) * 100\n",
    "            print(f\"{self.report['urls_with_lastmod']} URLs have a <lastmod> timestamp ({lastmod_percent:.2f}% coverage).\")\n",
    "\n",
    "        if self.report[\"recommendations\"]:\n",
    "            print_subheader(\"Recommendations (Based on ARI v10.0)\")\n",
    "            for rec in self.report[\"recommendations\"]:\n",
    "                print_recommendation(rec)\n",
    "\n",
    "        if self.report[\"error_log\"]:\n",
    "            print_subheader(\"Error Log\")\n",
    "            for err in self.report[\"error_log\"]:\n",
    "                print(f\"  - {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1541eda2-e68d-4f6e-aeb1-9fc0e1ed63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter website url :::  slack.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Sitemap Health & Freshness analysis for: slack.com\n",
      "\n",
      "\n",
      "============================================================\n",
      " ARI Sub-Pillar 1.1: Sitemap Health & Freshness\n",
      "============================================================\n",
      "Checking robots.txt at https://slack.com/robots.txt [IN PROGRESS]\n",
      "Found 2 sitemap(s) in robots.txt              [OK]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Processing Sitemaps\n",
      "------------------------------------------------------------\n",
      "\n",
      "-> Fetching: https://slack.com/sitemap.xml\n",
      "Parsed sitemap index: Found 13 more sitemaps  [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/blog/news/sitemap.xml\n",
      "Parsed URL set: Found 1 URLs                  [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_en-us.xml\n",
      "Parsed URL set: Found 3785 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_es-la.xml\n",
      "Parsed URL set: Found 3193 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_pt-br.xml\n",
      "Parsed URL set: Found 3157 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_de-de.xml\n",
      "Parsed URL set: Found 3281 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_es-es.xml\n",
      "Parsed URL set: Found 3102 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_fr-fr.xml\n",
      "Parsed URL set: Found 3291 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_it-it.xml\n",
      "Parsed URL set: Found 3292 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_en-gb.xml\n",
      "Parsed URL set: Found 3106 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_zh-cn.xml\n",
      "Parsed URL set: Found 3223 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_zh-tw.xml\n",
      "Parsed URL set: Found 3223 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_en-in.xml\n",
      "Parsed URL set: Found 3095 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_ja-jp.xml\n",
      "Parsed URL set: Found 3326 URLs               [INFO]\n",
      "\n",
      "-> Fetching: https://slack.com/sitemaps/sitemap_ko-kr.xml\n",
      "Parsed URL set: Found 2937 URLs               [INFO]\n",
      "\n",
      "============================================================\n",
      " Final Assessment Report\n",
      "============================================================\n",
      "Overall Status                                [Needs Improvement]\n",
      "ARI Score (out of 100)                        [50]\n",
      "\n",
      "------------------------------------------------------------\n",
      " Summary\n",
      "------------------------------------------------------------\n",
      "Discovered and processed 15 sitemap file(s).\n",
      "Found a total of 42012 URLs.\n",
      "0 URLs have a <lastmod> timestamp (0.00% coverage).\n",
      "\n",
      "------------------------------------------------------------\n",
      " Recommendations (Based on ARI v10.0)\n",
      "------------------------------------------------------------\n",
      "  - Improve coverage of <lastmod> timestamps for all URLs to signal content freshness.\n",
      "  - Consider implementing automatic sitemap updates in a CI/CD pipeline to keep timestamps current.\n",
      "\n",
      "============================================================\n",
      "SUMMARY OF OUTCOME\n",
      "============================================================\n",
      "Status: Needs Improvement\n",
      "Score: 50/100\n",
      "Total Sitemap Files Processed: 15\n",
      "Total URLs Found: 42012\n",
      "URLs with <lastmod>: 0\n",
      "\n",
      "============================================================\n",
      "MISSING POINTS / RECOMMENDATIONS\n",
      "============================================================\n",
      " - Improve coverage of <lastmod> timestamps for all URLs to signal content freshness.\n",
      " - Consider implementing automatic sitemap updates in a CI/CD pipeline to keep timestamps current.\n",
      "\n",
      "Sitemap URLs found (for further inspection):\n",
      " - https://slack.com/sitemap.xml\n",
      " - https://slack.com/sitemaps/blog/news/sitemap.xml\n",
      " - https://slack.com/sitemaps/sitemap_en-us.xml\n",
      " - https://slack.com/sitemaps/sitemap_es-la.xml\n",
      " - https://slack.com/sitemaps/sitemap_pt-br.xml\n",
      " - https://slack.com/sitemaps/sitemap_de-de.xml\n",
      " - https://slack.com/sitemaps/sitemap_es-es.xml\n",
      " - https://slack.com/sitemaps/sitemap_fr-fr.xml\n",
      " - https://slack.com/sitemaps/sitemap_it-it.xml\n",
      " - https://slack.com/sitemaps/sitemap_en-gb.xml\n",
      " - https://slack.com/sitemaps/sitemap_zh-cn.xml\n",
      " - https://slack.com/sitemaps/sitemap_zh-tw.xml\n",
      " - https://slack.com/sitemaps/sitemap_en-in.xml\n",
      " - https://slack.com/sitemaps/sitemap_ja-jp.xml\n",
      " - https://slack.com/sitemaps/sitemap_ko-kr.xml\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION CALL TO RUN ANALYSIS \"https://www.example.com\"\n",
    "target_url= input('Enter website url ::: ')\n",
    "print(f\"\\nRunning Sitemap Health & Freshness analysis for: {target_url}\\n\")\n",
    "# Initialize analyzer\n",
    "analyzer = SitemapAnalyzer(base_url=target_url)\n",
    "# Run full analysis\n",
    "analyzer.run_analysis()\n",
    "\n",
    "# Outcome Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF OUTCOME\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Status: {analyzer.report['status']}\")\n",
    "print(f\"Score: {analyzer.report['score']}/100\")\n",
    "print(f\"Total Sitemap Files Processed: {len(analyzer.report['sitemap_locations'])}\")\n",
    "print(f\"Total URLs Found: {analyzer.report['total_urls']}\")\n",
    "print(f\"URLs with <lastmod>: {analyzer.report['urls_with_lastmod']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING POINTS / RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "if analyzer.report[\"recommendations\"]:\n",
    "    for rec in analyzer.report[\"recommendations\"]:\n",
    "        print(f\" - {rec}\")\n",
    "else:\n",
    "    print(\"No immediate recommendations. Sitemap appears healthy.\")\n",
    "\n",
    " # Errors / Parsing Issues\n",
    "if analyzer.report[\"error_log\"]:\n",
    "    print(\"\\nErrors encountered during analysis:\")\n",
    "    for err in analyzer.report[\"error_log\"]:\n",
    "        print(f\" - {err}\")\n",
    "\n",
    "# Provide Sitemap URLs for Inspection\n",
    "if analyzer.report[\"sitemap_locations\"]:\n",
    "    print(\"\\nSitemap URLs found (for further inspection):\")\n",
    "    for sm in analyzer.report[\"sitemap_locations\"]:\n",
    "        print(f\" - {sm}\")\n",
    "else:\n",
    "    print(\"\\nNo sitemap URLs found. Consider creating sitemap.xml or specifying robots.txt sitemap.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c9a9d-6abd-45c6-9532-b7d1b47c8a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b943e4-6d67-4be7-9001-4588272a0137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07232e-d338-47eb-a82e-f806d838f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------WHAT IS MISSING -------------------------------\n",
    "1. Namespace Handling Could Be More Robust\n",
    "\n",
    "    Currently, you try to extract the namespace from the root tag:\n",
    "\n",
    "    namespace = root.tag.split('}')[0][1:] if '}' in root.tag else ''\n",
    "\n",
    "\n",
    "    This works for most sitemaps but may fail if XML uses multiple namespaces or no namespace.\n",
    "\n",
    "    Fix: Use ET.register_namespace() or a namespace dictionary for safer lookups.\n",
    "\n",
    "\n",
    "2. Handle Multiple Sitemap Formats\n",
    "\n",
    "    Only XML sitemaps are supported. Modern sites may also use:\n",
    "    \n",
    "    sitemap_index.xml (you partially handle this)\n",
    "    \n",
    "    .txt sitemaps (plain URL lists)\n",
    "    \n",
    "    RSS/Atom feeds acting as sitemaps\n",
    "    \n",
    "    Currently, .txt sitemaps will fail parsing.\n",
    "\n",
    "\n",
    "3. Validation of URLs\n",
    "\n",
    "    You count URLs but don’t validate if they are properly formatted.\n",
    "    \n",
    "    Missing check: ensure URLs are HTTP/HTTPS and belong to the same domain or intended scope.\n",
    "    \n",
    "    Could flag external URLs if they appear in the sitemap.\n",
    "\n",
    "4. Freshness / Lastmod Accuracy\n",
    "\n",
    "    You count <lastmod> but don’t verify timestamp validity.\n",
    "    \n",
    "    Could check:\n",
    "    \n",
    "    Correct ISO 8601 format\n",
    "    \n",
    "    Reasonable date (not in the future)\n",
    "\n",
    "5. Duplicate URLs\n",
    "\n",
    "    Currently, duplicates in sitemaps may inflate the URL count.\n",
    "    \n",
    "    Should deduplicate URLs before scoring.\n",
    "\n",
    "6. Logging & Exception Handling\n",
    "\n",
    "    You append errors to error_log, which is good.\n",
    "    \n",
    "    Missing: differentiating between critical errors vs warnings (e.g., empty sitemap vs minor parse warning).\n",
    "    \n",
    "    Could add log_level or categorize errors.\n",
    "\n",
    "7. Score Calculation\n",
    "\n",
    "    You calculate score based on lastmod coverage only.\n",
    "    \n",
    "    ARI v10.0 may also consider:\n",
    "    \n",
    "    Total sitemap discovery\n",
    "    \n",
    "    Accessibility of all URLs\n",
    "    \n",
    "    HTTP response codes for URLs\n",
    "    \n",
    "    Consistency with robots.txt directives\n",
    "    \n",
    "    Could improve scoring to reflect these factors.\n",
    "\n",
    "8. Recommendations / Best Practices\n",
    "\n",
    "    You suggest <lastmod> coverage improvement, which is good.\n",
    "    \n",
    "    Missing:\n",
    "    \n",
    "    Recommend using ping to search engines when sitemap updates.\n",
    "    \n",
    "    Suggest gzip compression for large sitemaps.\n",
    "    \n",
    "    Suggest splitting large sitemaps (>50k URLs) into multiple sitemaps.\n",
    "\n",
    "9. CI/CD / Automation\n",
    "\n",
    "     mention it in recommendations, but the analyzer does not detect automated update setup.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ef05d-d509-4eaa-a4d7-d247b0dcc828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b623c-c86e-4153-aeb5-620d1015565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------Summary of Key Missing Pieces----------------------------------\n",
    "| Area                 | Missing / Improvement                                           |\n",
    "| -------------------- | --------------------------------------------------------------- |\n",
    "| Namespace Handling   | More robust support for multiple namespaces                     |\n",
    "| Sitemap Formats      | Support `.txt` and feed-based sitemaps                          |\n",
    "| URL Validation       | Check proper URL formatting and domain scope                    |\n",
    "| Lastmod Verification | Validate ISO format and realistic timestamps                    |\n",
    "| Duplicates           | Deduplicate URLs before scoring                                 |\n",
    "| Logging              | Differentiate critical vs warning errors                        |\n",
    "| Scoring              | Include accessibility, robots.txt compliance, sitemap discovery |\n",
    "| Recommendations      | Include size, gzip, search engine ping, sitemap splitting       |\n",
    "| Automation Detection | Optional check for CI/CD sitemap updates                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c87b591-bd86-44af-9dec-8b942fe0b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Helper functions for formatted output\n",
    "def print_header(text):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\" {text}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "def print_subheader(text):\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\" {text}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "def print_status(message, status):\n",
    "    padded_message = f\"{message:<50}\"\n",
    "    if status in [\"ERROR\", \"CRITICAL\"]:\n",
    "        status_str = f\"[\\033[91m{status}\\033[0m]\"\n",
    "    elif status == \"WARNING\":\n",
    "        status_str = f\"[\\033[93m{status}\\033[0m]\"\n",
    "    elif status in [\"OK\", \"PASS\", \"INFO\"]:\n",
    "        status_str = f\"[\\033[92m{status}\\033[0m]\"\n",
    "    else:\n",
    "        status_str = f\"[{status}]\"\n",
    "    print(f\"{padded_message} {status_str}\")\n",
    "\n",
    "def print_recommendation(rec):\n",
    "    print(f\"  → {rec}\")\n",
    "\n",
    "class EnhancedSitemapAnalyzer:\n",
    "    \"\"\"\n",
    "    Enhanced analyzer for ARI v10.0 Pillar 1, Sub-pillar 1.\n",
    "    Comprehensive sitemap health, freshness, and compliance checking.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = self._format_base_url(base_url)\n",
    "        self.base_domain = urlparse(self.base_url).netloc\n",
    "        self.sitemaps_to_process = []\n",
    "        self.processed_sitemaps = set()\n",
    "        self.all_urls = set()  # For deduplication\n",
    "        self.url_details = []  # Store URL metadata\n",
    "        \n",
    "        self.report = {\n",
    "            \"sitemap_locations\": [],\n",
    "            \"total_urls\": 0,\n",
    "            \"unique_urls\": 0,\n",
    "            \"duplicate_urls\": 0,\n",
    "            \"urls_with_lastmod\": 0,\n",
    "            \"urls_with_valid_lastmod\": 0,\n",
    "            \"urls_with_invalid_format\": 0,\n",
    "            \"external_urls\": 0,\n",
    "            \"future_dated_urls\": 0,\n",
    "            \"txt_sitemaps_found\": 0,\n",
    "            \"xml_sitemaps_found\": 0,\n",
    "            \"gzipped_sitemaps\": 0,\n",
    "            \"oversized_sitemaps\": 0,\n",
    "            \"robots_txt_found\": False,\n",
    "            \"sitemap_in_robots\": False,\n",
    "            \"critical_errors\": [],\n",
    "            \"warnings\": [],\n",
    "            \"error_log\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"score\": 0,\n",
    "            \"status\": \"Not Assessed\"\n",
    "        }\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'ARI-Enhanced-Sitemap-Analyzer/2.0',\n",
    "            'Accept-Encoding': 'gzip, deflate'\n",
    "        })\n",
    "\n",
    "    def _format_base_url(self, url):\n",
    "        \"\"\"Normalize base URL with proper scheme.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        if not parsed.scheme:\n",
    "            url = \"https://\" + url\n",
    "            parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "    def _fetch_url(self, url):\n",
    "        \"\"\"Fetch URL with comprehensive error handling.\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            self.report[\"error_log\"].append(f\"Timeout fetching {url}\")\n",
    "            return None\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            self.report[\"error_log\"].append(f\"HTTP {e.response.status_code} for {url}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.report[\"error_log\"].append(f\"Network error for {url}: {str(e)[:100]}\")\n",
    "            return None\n",
    "\n",
    "    def _get_sitemap_content(self, response):\n",
    "        \"\"\"Handle compressed and uncompressed sitemap content.\"\"\"\n",
    "        # Check if gzipped\n",
    "        is_gzipped = (\n",
    "            response.url.endswith('.gz') or \n",
    "            'gzip' in response.headers.get('Content-Encoding', '').lower() or\n",
    "            response.headers.get('Content-Type') == 'application/gzip'\n",
    "        )\n",
    "        \n",
    "        if is_gzipped:\n",
    "            self.report[\"gzipped_sitemaps\"] += 1\n",
    "            try:\n",
    "                with gzip.GzipFile(fileobj=BytesIO(response.content)) as gz_file:\n",
    "                    return gz_file.read().decode('utf-8')\n",
    "            except Exception as e:\n",
    "                self.report[\"critical_errors\"].append(\n",
    "                    f\"Failed to decompress gzipped sitemap {response.url}: {str(e)[:100]}\"\n",
    "                )\n",
    "                return None\n",
    "        \n",
    "        return response.text\n",
    "\n",
    "    def _validate_url(self, url):\n",
    "        \"\"\"\n",
    "        Validate URL format and scope.\n",
    "        Returns: (is_valid, is_external, issues)\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Basic format check\n",
    "        if not url or not isinstance(url, str):\n",
    "            return False, False, [\"Empty or invalid URL type\"]\n",
    "        \n",
    "        # Parse URL\n",
    "        try:\n",
    "            parsed = urlparse(url.strip())\n",
    "        except Exception:\n",
    "            return False, False, [\"URL parsing failed\"]\n",
    "        \n",
    "        # Check scheme\n",
    "        if parsed.scheme not in ['http', 'https']:\n",
    "            issues.append(f\"Invalid scheme: {parsed.scheme}\")\n",
    "            return False, False, issues\n",
    "        \n",
    "        # Check if external\n",
    "        is_external = parsed.netloc.lower() != self.base_domain.lower()\n",
    "        \n",
    "        # Check for suspicious patterns\n",
    "        if '..' in parsed.path or parsed.path.startswith('//'):\n",
    "            issues.append(\"Suspicious path pattern\")\n",
    "        \n",
    "        return True, is_external, issues\n",
    "\n",
    "    def _validate_lastmod(self, lastmod_text):\n",
    "        \"\"\"\n",
    "        Validate lastmod timestamp format and reasonableness.\n",
    "        Returns: (is_valid, parsed_datetime, issues)\n",
    "        \"\"\"\n",
    "        if not lastmod_text:\n",
    "            return False, None, [\"Empty lastmod\"]\n",
    "        \n",
    "        issues = []\n",
    "        \n",
    "        # Common ISO 8601 formats\n",
    "        formats = [\n",
    "            \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "            \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            \"%Y-%m-%d\",\n",
    "            \"%Y-%m-%dT%H:%M:%S\",\n",
    "        ]\n",
    "        \n",
    "        parsed_dt = None\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                # Handle 'Z' timezone marker\n",
    "                lastmod_clean = lastmod_text.strip().replace('Z', '+00:00')\n",
    "                # Handle timezone format like +00:00\n",
    "                if '+' in lastmod_clean or lastmod_clean.count('-') > 2:\n",
    "                    parsed_dt = datetime.fromisoformat(lastmod_clean)\n",
    "                else:\n",
    "                    parsed_dt = datetime.strptime(lastmod_clean, fmt)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        if not parsed_dt:\n",
    "            return False, None, [\"Invalid date format\"]\n",
    "        \n",
    "        # Check if date is in the future\n",
    "        now = datetime.now(timezone.utc)\n",
    "        if parsed_dt.tzinfo is None:\n",
    "            parsed_dt = parsed_dt.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        if parsed_dt > now:\n",
    "            issues.append(\"Future-dated timestamp\")\n",
    "            return False, parsed_dt, issues\n",
    "        \n",
    "        # Check if date is unreasonably old (before 1995 - start of WWW)\n",
    "        if parsed_dt.year < 1995:\n",
    "            issues.append(\"Unreasonably old date\")\n",
    "            return False, parsed_dt, issues\n",
    "        \n",
    "        return True, parsed_dt, issues\n",
    "\n",
    "    def find_sitemaps_from_robots(self):\n",
    "        \"\"\"Parse robots.txt to discover sitemap locations.\"\"\"\n",
    "        print_status(\"Checking robots.txt\", \"INFO\")\n",
    "        robots_url = urljoin(self.base_url, 'robots.txt')\n",
    "        response = self._fetch_url(robots_url)\n",
    "\n",
    "        if response:\n",
    "            self.report[\"robots_txt_found\"] = True\n",
    "            lines = response.text.splitlines()\n",
    "            found_count = 0\n",
    "            \n",
    "            for line in lines:\n",
    "                line_clean = line.strip()\n",
    "                if line_clean.lower().startswith('sitemap:'):\n",
    "                    sitemap_url = line_clean.split(':', 1)[1].strip()\n",
    "                    self.sitemaps_to_process.append(sitemap_url)\n",
    "                    found_count += 1\n",
    "            \n",
    "            if found_count > 0:\n",
    "                self.report[\"sitemap_in_robots\"] = True\n",
    "                print_status(f\"Found {found_count} sitemap(s) in robots.txt\", \"OK\")\n",
    "            else:\n",
    "                print_status(\"No sitemap directive in robots.txt\", \"WARNING\")\n",
    "                self.report[\"warnings\"].append(\n",
    "                    \"robots.txt exists but contains no Sitemap: directive\"\n",
    "                )\n",
    "        else:\n",
    "            print_status(\"Could not fetch robots.txt\", \"WARNING\")\n",
    "            self.report[\"warnings\"].append(\"robots.txt not found or inaccessible\")\n",
    "\n",
    "    def _parse_txt_sitemap(self, content, sitemap_url):\n",
    "        \"\"\"Parse plain text sitemap (one URL per line).\"\"\"\n",
    "        print_status(\"Parsing as TXT sitemap\", \"INFO\")\n",
    "        self.report[\"txt_sitemaps_found\"] += 1\n",
    "        \n",
    "        lines = content.strip().split('\\n')\n",
    "        valid_urls = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            url = line.strip()\n",
    "            if not url or url.startswith('#'):  # Skip empty lines and comments\n",
    "                continue\n",
    "            \n",
    "            is_valid, is_external, issues = self._validate_url(url)\n",
    "            \n",
    "            if is_valid:\n",
    "                if url not in self.all_urls:\n",
    "                    self.all_urls.add(url)\n",
    "                    self.url_details.append({\n",
    "                        'url': url,\n",
    "                        'lastmod': None,\n",
    "                        'is_external': is_external,\n",
    "                        'source_sitemap': sitemap_url\n",
    "                    })\n",
    "                    valid_urls += 1\n",
    "                else:\n",
    "                    self.report[\"duplicate_urls\"] += 1\n",
    "                \n",
    "                if is_external:\n",
    "                    self.report[\"external_urls\"] += 1\n",
    "            else:\n",
    "                self.report[\"urls_with_invalid_format\"] += 1\n",
    "        \n",
    "        print_status(f\"Found {valid_urls} valid URLs in TXT sitemap\", \"OK\")\n",
    "\n",
    "    def _parse_xml_sitemap(self, xml_content, sitemap_url):\n",
    "        \"\"\"Parse XML sitemap with robust namespace handling.\"\"\"\n",
    "        try:\n",
    "            root = ET.fromstring(xml_content.encode('utf-8'))\n",
    "        except ET.ParseError as e:\n",
    "            self.report[\"critical_errors\"].append(\n",
    "                f\"XML Parse Error in {sitemap_url}: {str(e)[:100]}\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # Extract namespace more robustly\n",
    "        namespace_match = re.match(r'\\{(.*?)\\}', root.tag)\n",
    "        namespace = namespace_match.group(1) if namespace_match else ''\n",
    "        \n",
    "        # Define namespace map for searching\n",
    "        ns = {'sm': namespace} if namespace else {}\n",
    "        \n",
    "        # Check if it's a sitemap index\n",
    "        if root.tag.endswith('sitemapindex'):\n",
    "            self._parse_sitemap_index(root, ns, sitemap_url)\n",
    "        \n",
    "        # Check if it's a URL set\n",
    "        elif root.tag.endswith('urlset'):\n",
    "            self._parse_urlset(root, ns, sitemap_url)\n",
    "        \n",
    "        else:\n",
    "            self.report[\"warnings\"].append(\n",
    "                f\"Unknown XML root tag '{root.tag}' in {sitemap_url}\"\n",
    "            )\n",
    "\n",
    "    def _parse_sitemap_index(self, root, ns, sitemap_url):\n",
    "        \"\"\"Parse sitemap index file.\"\"\"\n",
    "        if ns:\n",
    "            sitemaps = root.findall('.//sm:sitemap', ns)\n",
    "        else:\n",
    "            sitemaps = root.findall('.//sitemap')\n",
    "        \n",
    "        for sitemap in sitemaps:\n",
    "            if ns:\n",
    "                loc = sitemap.find('sm:loc', ns)\n",
    "            else:\n",
    "                loc = sitemap.find('loc')\n",
    "            \n",
    "            if loc is not None and loc.text:\n",
    "                new_sitemap_url = loc.text.strip()\n",
    "                if new_sitemap_url not in self.processed_sitemaps:\n",
    "                    self.sitemaps_to_process.append(new_sitemap_url)\n",
    "        \n",
    "        print_status(f\"Sitemap index: Found {len(sitemaps)} sub-sitemaps\", \"INFO\")\n",
    "\n",
    "    def _parse_urlset(self, root, ns, sitemap_url):\n",
    "        \"\"\"Parse URL set with comprehensive validation.\"\"\"\n",
    "        self.report[\"xml_sitemaps_found\"] += 1\n",
    "        \n",
    "        if ns:\n",
    "            urls = root.findall('.//sm:url', ns)\n",
    "        else:\n",
    "            urls = root.findall('.//url')\n",
    "        \n",
    "        # Check for oversized sitemap (>50k URLs per Google guidelines)\n",
    "        if len(urls) > 50000:\n",
    "            self.report[\"oversized_sitemaps\"] += 1\n",
    "            self.report[\"warnings\"].append(\n",
    "                f\"Sitemap {sitemap_url} contains {len(urls)} URLs (>50k limit)\"\n",
    "            )\n",
    "        \n",
    "        for url_elem in urls:\n",
    "            # Extract loc\n",
    "            if ns:\n",
    "                loc = url_elem.find('sm:loc', ns)\n",
    "                lastmod = url_elem.find('sm:lastmod', ns)\n",
    "            else:\n",
    "                loc = url_elem.find('loc')\n",
    "                lastmod = url_elem.find('lastmod')\n",
    "            \n",
    "            if loc is None or not loc.text:\n",
    "                self.report[\"urls_with_invalid_format\"] += 1\n",
    "                continue\n",
    "            \n",
    "            url = loc.text.strip()\n",
    "            \n",
    "            # Validate URL\n",
    "            is_valid, is_external, url_issues = self._validate_url(url)\n",
    "            \n",
    "            if not is_valid:\n",
    "                self.report[\"urls_with_invalid_format\"] += 1\n",
    "                for issue in url_issues:\n",
    "                    self.report[\"warnings\"].append(f\"URL {url}: {issue}\")\n",
    "                continue\n",
    "            \n",
    "            # Check for duplicates\n",
    "            if url in self.all_urls:\n",
    "                self.report[\"duplicate_urls\"] += 1\n",
    "                continue\n",
    "            \n",
    "            self.all_urls.add(url)\n",
    "            \n",
    "            # Track external URLs\n",
    "            if is_external:\n",
    "                self.report[\"external_urls\"] += 1\n",
    "                self.report[\"warnings\"].append(f\"External URL in sitemap: {url}\")\n",
    "            \n",
    "            # Validate lastmod\n",
    "            lastmod_valid = False\n",
    "            lastmod_datetime = None\n",
    "            \n",
    "            if lastmod is not None and lastmod.text:\n",
    "                self.report[\"urls_with_lastmod\"] += 1\n",
    "                is_valid_date, parsed_dt, date_issues = self._validate_lastmod(lastmod.text)\n",
    "                \n",
    "                if is_valid_date:\n",
    "                    self.report[\"urls_with_valid_lastmod\"] += 1\n",
    "                    lastmod_valid = True\n",
    "                    lastmod_datetime = parsed_dt\n",
    "                else:\n",
    "                    for issue in date_issues:\n",
    "                        if \"Future-dated\" in issue:\n",
    "                            self.report[\"future_dated_urls\"] += 1\n",
    "                        self.report[\"warnings\"].append(f\"URL {url} lastmod: {issue}\")\n",
    "            \n",
    "            # Store URL details\n",
    "            self.url_details.append({\n",
    "                'url': url,\n",
    "                'lastmod': lastmod_datetime,\n",
    "                'lastmod_valid': lastmod_valid,\n",
    "                'is_external': is_external,\n",
    "                'source_sitemap': sitemap_url\n",
    "            })\n",
    "        \n",
    "        self.report[\"unique_urls\"] = len(self.all_urls)\n",
    "        self.report[\"total_urls\"] = len(self.url_details)\n",
    "        print_status(f\"Parsed {len(urls)} URLs from sitemap\", \"OK\")\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Main execution logic.\"\"\"\n",
    "        print_header(\"ARI v10.0 Enhanced Sitemap Analyzer\")\n",
    "        print(\"Pillar 1, Sub-pillar 1: Sitemap Health & Freshness\\n\")\n",
    "\n",
    "        # Step 1: Discover sitemaps from robots.txt\n",
    "        self.find_sitemaps_from_robots()\n",
    "        \n",
    "        # Step 2: Fallback to default sitemap locations\n",
    "        if not self.sitemaps_to_process:\n",
    "            print_status(\"Trying default sitemap locations\", \"INFO\")\n",
    "            default_locations = [\n",
    "                urljoin(self.base_url, 'sitemap.xml'),\n",
    "                urljoin(self.base_url, 'sitemap_index.xml'),\n",
    "                urljoin(self.base_url, 'sitemap.txt'),\n",
    "            ]\n",
    "            self.sitemaps_to_process.extend(default_locations)\n",
    "\n",
    "        # Step 3: Process all sitemaps\n",
    "        print_subheader(\"Processing Sitemaps\")\n",
    "        \n",
    "        while self.sitemaps_to_process:\n",
    "            sitemap_url = self.sitemaps_to_process.pop(0)\n",
    "            \n",
    "            if sitemap_url in self.processed_sitemaps:\n",
    "                continue\n",
    "            \n",
    "            self.processed_sitemaps.add(sitemap_url)\n",
    "            print(f\"\\n→ Fetching: {sitemap_url}\")\n",
    "            \n",
    "            response = self._fetch_url(sitemap_url)\n",
    "            \n",
    "            if not response:\n",
    "                print_status(\"Failed to fetch sitemap\", \"ERROR\")\n",
    "                continue\n",
    "            \n",
    "            self.report[\"sitemap_locations\"].append(sitemap_url)\n",
    "            \n",
    "            # Get content (handles gzip)\n",
    "            content = self._get_sitemap_content(response)\n",
    "            if not content:\n",
    "                print_status(\"Failed to read sitemap content\", \"ERROR\")\n",
    "                continue\n",
    "            \n",
    "            # Determine format and parse\n",
    "            if sitemap_url.endswith('.txt') or not content.strip().startswith('<'):\n",
    "                self._parse_txt_sitemap(content, sitemap_url)\n",
    "            else:\n",
    "                self._parse_xml_sitemap(content, sitemap_url)\n",
    "\n",
    "        # Step 4: Generate report\n",
    "        self._generate_final_report()\n",
    "        self._print_final_report()\n",
    "\n",
    "    def _generate_final_report(self):\n",
    "        \"\"\"Calculate score and generate recommendations.\"\"\"\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # Critical Failures\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        if not self.report[\"sitemap_locations\"]:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 0\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"CRITICAL: No sitemap found. Generate sitemap.xml immediately\"\n",
    "            )\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Submit sitemap to Google Search Console and Bing Webmaster Tools\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        if self.report[\"critical_errors\"]:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 10\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"CRITICAL: Fix XML parsing errors in sitemap\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        if self.report[\"total_urls\"] == 0:\n",
    "            self.report[\"status\"] = \"Critical Failure\"\n",
    "            self.report[\"score\"] = 15\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"CRITICAL: Sitemap is empty or all URLs are invalid\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # Scoring (100 points total)\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        score = 0\n",
    "        \n",
    "        # 1. Sitemap Discovery (20 points)\n",
    "        if self.report[\"robots_txt_found\"]:\n",
    "            score += 10\n",
    "        if self.report[\"sitemap_in_robots\"]:\n",
    "            score += 10\n",
    "        \n",
    "        # 2. URL Quality (30 points)\n",
    "        if self.report[\"unique_urls\"] > 0:\n",
    "            duplicate_ratio = self.report[\"duplicate_urls\"] / (self.report[\"unique_urls\"] + self.report[\"duplicate_urls\"])\n",
    "            invalid_ratio = self.report[\"urls_with_invalid_format\"] / max(1, self.report[\"total_urls\"])\n",
    "            external_ratio = self.report[\"external_urls\"] / max(1, self.report[\"total_urls\"])\n",
    "            \n",
    "            url_quality = (1 - duplicate_ratio) * (1 - invalid_ratio) * (1 - external_ratio)\n",
    "            score += int(30 * url_quality)\n",
    "        \n",
    "        # 3. Lastmod Coverage (25 points)\n",
    "        if self.report[\"total_urls\"] > 0:\n",
    "            lastmod_coverage = self.report[\"urls_with_lastmod\"] / self.report[\"total_urls\"]\n",
    "            score += int(25 * lastmod_coverage)\n",
    "        \n",
    "        # 4. Lastmod Validity (15 points)\n",
    "        if self.report[\"urls_with_lastmod\"] > 0:\n",
    "            lastmod_validity = self.report[\"urls_with_valid_lastmod\"] / self.report[\"urls_with_lastmod\"]\n",
    "            score += int(15 * lastmod_validity)\n",
    "        \n",
    "        # 5. Best Practices (10 points)\n",
    "        best_practices = 0\n",
    "        if self.report[\"gzipped_sitemaps\"] > 0:\n",
    "            best_practices += 5  # Using compression\n",
    "        if self.report[\"oversized_sitemaps\"] == 0:\n",
    "            best_practices += 5  # No oversized sitemaps\n",
    "        score += best_practices\n",
    "        \n",
    "        self.report[\"score\"] = min(100, int(score))\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # Status Assignment\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        if self.report[\"score\"] >= 90:\n",
    "            self.report[\"status\"] = \"Excellent\"\n",
    "        elif self.report[\"score\"] >= 75:\n",
    "            self.report[\"status\"] = \"Good\"\n",
    "        elif self.report[\"score\"] >= 50:\n",
    "            self.report[\"status\"] = \"Needs Improvement\"\n",
    "        else:\n",
    "            self.report[\"status\"] = \"Poor\"\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # Generate Recommendations\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        if not self.report[\"robots_txt_found\"]:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Create robots.txt file at domain root\"\n",
    "            )\n",
    "        \n",
    "        if not self.report[\"sitemap_in_robots\"]:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Add 'Sitemap: <URL>' directive to robots.txt\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"duplicate_urls\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                f\"Remove {self.report['duplicate_urls']} duplicate URLs from sitemap(s)\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"urls_with_invalid_format\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                f\"Fix {self.report['urls_with_invalid_format']} URLs with invalid format\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"external_urls\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                f\"Remove {self.report['external_urls']} external URLs from sitemap\"\n",
    "            )\n",
    "        \n",
    "        lastmod_coverage = (self.report[\"urls_with_lastmod\"] / self.report[\"total_urls\"]) if self.report[\"total_urls\"] > 0 else 0\n",
    "        if lastmod_coverage < 0.9:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                f\"Improve <lastmod> coverage to 90%+ (currently {lastmod_coverage*100:.1f}%)\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"future_dated_urls\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                f\"Fix {self.report['future_dated_urls']} future-dated timestamps\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"oversized_sitemaps\"] > 0:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Split oversized sitemaps (>50k URLs) into multiple files with sitemap index\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"gzipped_sitemaps\"] == 0 and self.report[\"total_urls\"] > 1000:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Enable gzip compression for sitemaps to reduce bandwidth\"\n",
    "            )\n",
    "        \n",
    "        if self.report[\"score\"] >= 70:\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Implement automated sitemap updates in CI/CD pipeline\"\n",
    "            )\n",
    "            self.report[\"recommendations\"].append(\n",
    "                \"Set up automatic ping to search engines on sitemap updates\"\n",
    "            )\n",
    "\n",
    "    def _print_final_report(self):\n",
    "        \"\"\"Print comprehensive final report.\"\"\"\n",
    "        print_header(\"Final Assessment Report\")\n",
    "        \n",
    "        # Overall Status\n",
    "        print_status(\"Overall Status\", self.report['status'])\n",
    "        print_status(f\"ARI Score (out of 100)\", str(self.report['score']))\n",
    "        \n",
    "        # Discovery Summary\n",
    "        print_subheader(\"Sitemap Discovery\")\n",
    "        print(f\"  Sitemaps Processed: {len(self.report['sitemap_locations'])}\")\n",
    "        print(f\"  XML Sitemaps: {self.report['xml_sitemaps_found']}\")\n",
    "        print(f\"  TXT Sitemaps: {self.report['txt_sitemaps_found']}\")\n",
    "        print(f\"  Gzipped Sitemaps: {self.report['gzipped_sitemaps']}\")\n",
    "        print(f\"  robots.txt Found: {'Yes' if self.report['robots_txt_found'] else 'No'}\")\n",
    "        print(f\"  Sitemap in robots.txt: {'Yes' if self.report['sitemap_in_robots'] else 'No'}\")\n",
    "        \n",
    "        # URL Statistics\n",
    "        print_subheader(\"URL Statistics\")\n",
    "        print(f\"  Total URLs Found: {self.report['total_urls']}\")\n",
    "        print(f\"  Unique URLs: {self.report['unique_urls']}\")\n",
    "        print(f\"  Duplicate URLs: {self.report['duplicate_urls']}\")\n",
    "        print(f\"  Invalid Format URLs: {self.report['urls_with_invalid_format']}\")\n",
    "        print(f\"  External URLs: {self.report['external_urls']}\")\n",
    "        \n",
    "        # Lastmod Statistics\n",
    "        print_subheader(\"Timestamp Quality\")\n",
    "        if self.report['total_urls'] > 0:\n",
    "            lastmod_pct = (self.report['urls_with_lastmod'] / self.report['total_urls']) * 100\n",
    "            print(f\"  URLs with <lastmod>: {self.report['urls_with_lastmod']} ({lastmod_pct:.1f}%)\")\n",
    "        if self.report['urls_with_lastmod'] > 0:\n",
    "            valid_pct = (self.report['urls_with_valid_lastmod'] / self.report['urls_with_lastmod']) * 100\n",
    "            print(f\"  Valid Timestamps: {self.report['urls_with_valid_lastmod']} ({valid_pct:.1f}%)\")\n",
    "        print(f\"  Future-dated URLs: {self.report['future_dated_urls']}\")\n",
    "        \n",
    "        # Issues\n",
    "        if self.report[\"critical_errors\"]:\n",
    "            print_subheader(\"Critical Errors\")\n",
    "            for error in self.report[\"critical_errors\"][:5]:\n",
    "                print(f\"  ⚠️  {error}\")\n",
    "        \n",
    "        if self.report[\"warnings\"]:\n",
    "            print_subheader(\"Warnings (Top 5)\")\n",
    "            for warning in self.report[\"warnings\"][:5]:\n",
    "                print(f\"  ⚡ {warning}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        if self.report[\"recommendations\"]:\n",
    "            print_subheader(\"Actionable Recommendations\")\n",
    "            for i, rec in enumerate(self.report[\"recommendations\"], 1):\n",
    "                print_recommendation(f\"{i}. {rec}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7824647f-f1dd-48cb-97dc-6ccad6bec1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedSitemapAnalyzer============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:   https://www.example.com/404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " ARI v10.0 Enhanced Sitemap Analyzer\n",
      "======================================================================\n",
      "Pillar 1, Sub-pillar 1: Sitemap Health & Freshness\n",
      "\n",
      "Checking robots.txt                                [\u001b[92mINFO\u001b[0m]\n",
      "Could not fetch robots.txt                         [\u001b[93mWARNING\u001b[0m]\n",
      "Trying default sitemap locations                   [\u001b[92mINFO\u001b[0m]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Processing Sitemaps\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "→ Fetching: https://www.example.com/sitemap.xml\n",
      "Failed to fetch sitemap                            [\u001b[91mERROR\u001b[0m]\n",
      "\n",
      "→ Fetching: https://www.example.com/sitemap_index.xml\n",
      "Failed to fetch sitemap                            [\u001b[91mERROR\u001b[0m]\n",
      "\n",
      "→ Fetching: https://www.example.com/sitemap.txt\n",
      "Failed to fetch sitemap                            [\u001b[91mERROR\u001b[0m]\n",
      "\n",
      "======================================================================\n",
      " Final Assessment Report\n",
      "======================================================================\n",
      "Overall Status                                     [Critical Failure]\n",
      "ARI Score (out of 100)                             [0]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Sitemap Discovery\n",
      "----------------------------------------------------------------------\n",
      "  Sitemaps Processed: 0\n",
      "  XML Sitemaps: 0\n",
      "  TXT Sitemaps: 0\n",
      "  Gzipped Sitemaps: 0\n",
      "  robots.txt Found: No\n",
      "  Sitemap in robots.txt: No\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " URL Statistics\n",
      "----------------------------------------------------------------------\n",
      "  Total URLs Found: 0\n",
      "  Unique URLs: 0\n",
      "  Duplicate URLs: 0\n",
      "  Invalid Format URLs: 0\n",
      "  External URLs: 0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Timestamp Quality\n",
      "----------------------------------------------------------------------\n",
      "  Future-dated URLs: 0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Warnings (Top 5)\n",
      "----------------------------------------------------------------------\n",
      "  ⚡ robots.txt not found or inaccessible\n",
      "\n",
      "----------------------------------------------------------------------\n",
      " Actionable Recommendations\n",
      "----------------------------------------------------------------------\n",
      "  → 1. CRITICAL: No sitemap found. Generate sitemap.xml immediately\n",
      "  → 2. Submit sitemap to Google Search Console and Bing Webmaster Tools\n",
      "\n",
      "============================================================\n",
      "RAW REPORT OBJECT\n",
      "============================================================\n",
      "{'sitemap_locations': [], 'total_urls': 0, 'unique_urls': 0, 'duplicate_urls': 0, 'urls_with_lastmod': 0, 'urls_with_valid_lastmod': 0, 'urls_with_invalid_format': 0, 'external_urls': 0, 'future_dated_urls': 0, 'txt_sitemaps_found': 0, 'xml_sitemaps_found': 0, 'gzipped_sitemaps': 0, 'oversized_sitemaps': 0, 'robots_txt_found': False, 'sitemap_in_robots': False, 'critical_errors': [], 'warnings': ['robots.txt not found or inaccessible'], 'error_log': ['HTTP 404 for https://www.example.com/robots.txt', 'HTTP 404 for https://www.example.com/sitemap.xml', 'HTTP 404 for https://www.example.com/sitemap_index.xml', 'HTTP 404 for https://www.example.com/sitemap.txt'], 'recommendations': ['CRITICAL: No sitemap found. Generate sitemap.xml immediately', 'Submit sitemap to Google Search Console and Bing Webmaster Tools'], 'score': 0, 'status': 'Critical Failure'}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# USAGE - Direct execution\n",
    "# ============================================================================\n",
    "\n",
    "print(\"EnhancedSitemapAnalyzer\" + \"=\"*60)\n",
    "target_url = input(\"Enter URL: \").strip()\n",
    "\n",
    "# Instantiate the analyzer\n",
    "analyzer = EnhancedSitemapAnalyzer(base_url=target_url)\n",
    "\n",
    "# Run the analysis\n",
    "analyzer.run_analysis()\n",
    "\n",
    "# Print structured raw report for dev / API use\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW REPORT OBJECT\")\n",
    "print(\"=\"*60)\n",
    "print(analyzer.report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb63320-8ce3-4b6e-8cf8-816c872d72c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
